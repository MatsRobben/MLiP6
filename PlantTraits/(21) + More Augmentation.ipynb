{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1cfbaa9",
   "metadata": {
    "papermill": {
     "duration": 0.006764,
     "end_time": "2024-04-20T20:45:42.570870",
     "exception": false,
     "start_time": "2024-04-20T20:45:42.564106",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Notebook modified from https://www.kaggle.com/code/markwijkhuizen/planttraits2024-eda-training-pub.\n",
    "- Training only, EDA part not included.\n",
    "- Image model only, tabular data not used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832bb948",
   "metadata": {
    "papermill": {
     "duration": 0.004545,
     "end_time": "2024-04-20T20:45:42.580423",
     "exception": false,
     "start_time": "2024-04-20T20:45:42.575878",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1288adcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T20:45:42.591921Z",
     "iopub.status.busy": "2024-04-20T20:45:42.591138Z",
     "iopub.status.idle": "2024-04-20T20:45:55.679834Z",
     "shell.execute_reply": "2024-04-20T20:45:55.678869Z"
    },
    "papermill": {
     "duration": 13.097195,
     "end_time": "2024-04-20T20:45:55.682360",
     "exception": false,
     "start_time": "2024-04-20T20:45:42.585165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio.v3 as imageio\n",
    "import albumentations as A\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "\n",
    "import torch\n",
    "import timm\n",
    "import glob\n",
    "import torchmetrics\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "tqdm.pandas()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee00f64a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T20:45:55.693329Z",
     "iopub.status.busy": "2024-04-20T20:45:55.693007Z",
     "iopub.status.idle": "2024-04-20T20:45:55.699357Z",
     "shell.execute_reply": "2024-04-20T20:45:55.698447Z"
    },
    "papermill": {
     "duration": 0.014135,
     "end_time": "2024-04-20T20:45:55.701482",
     "exception": false,
     "start_time": "2024-04-20T20:45:55.687347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    IMAGE_SIZE = 384\n",
    "    BACKBONE = 'swin_large_patch4_window12_384.ms_in22k_ft_in1k'\n",
    "#     BACKBONE = 'tf_efficientnet_b0'\n",
    "    TARGET_COLUMNS = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n",
    "    \n",
    "    N_TARGETS = len(TARGET_COLUMNS)\n",
    "    BATCH_SIZE = 10\n",
    "    LR_MAX = 1e-4\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    N_EPOCHS = 7\n",
    "    TRAIN_MODEL = True\n",
    "    IS_INTERACTIVE = os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive'\n",
    "    \n",
    "#     MODEL_PATH = '/kaggle/input/plainttraits2024-swintransformer/model.pth'\n",
    "    MODEL_PATH = '/kaggle/input/planttraits2024-swintransformer-tabular/model.pth'\n",
    "        \n",
    "CONFIG = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ba90da",
   "metadata": {
    "papermill": {
     "duration": 0.004735,
     "end_time": "2024-04-20T20:45:55.710811",
     "exception": false,
     "start_time": "2024-04-20T20:45:55.706076",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "136dcefa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T20:45:55.721351Z",
     "iopub.status.busy": "2024-04-20T20:45:55.721057Z",
     "iopub.status.idle": "2024-04-20T20:46:22.091088Z",
     "shell.execute_reply": "2024-04-20T20:46:22.089922Z"
    },
    "papermill": {
     "duration": 26.378344,
     "end_time": "2024-04-20T20:46:22.093708",
     "exception": false,
     "start_time": "2024-04-20T20:45:55.715364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_TRAIN_SAMPLES: 49168 N_TEST_SAMPLES: 6545\n",
      "CPU times: user 1.83 s, sys: 3.14 s, total: 4.97 s\n",
      "Wall time: 26.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "read_images = False\n",
    "\n",
    "if not read_images:\n",
    "    train = pd.read_pickle('/kaggle/input/plainttraits2024-swintransformer/train.pkl')\n",
    "    test = pd.read_pickle('/kaggle/input/plainttraits2024-swintransformer/test.pkl')\n",
    "else: \n",
    "    # if CONFIG.TRAIN_MODEL:\n",
    "    train = pd.read_csv('/kaggle/input/planttraits2024/train.csv')\n",
    "    train['file_path'] = train['id'].apply(lambda s: f'/kaggle/input/planttraits2024/train_images/{s}.jpeg')\n",
    "    train['jpeg_bytes'] = train['file_path'].progress_apply(lambda fp: open(fp, 'rb').read())\n",
    "    train.to_pickle('train.pkl')\n",
    "\n",
    "    test = pd.read_csv('/kaggle/input/planttraits2024/test.csv')\n",
    "    test['file_path'] = test['id'].apply(lambda s: f'/kaggle/input/planttraits2024/test_images/{s}.jpeg')\n",
    "    test['jpeg_bytes'] = test['file_path'].progress_apply(lambda fp: open(fp, 'rb').read())\n",
    "    test.to_pickle('test.pkl')\n",
    "\n",
    "for column in CONFIG.TARGET_COLUMNS:\n",
    "    lower_quantile = train[column].quantile(0.005)\n",
    "    upper_quantile = train[column].quantile(0.985)  \n",
    "    train = train[(train[column] >= lower_quantile) & (train[column] <= upper_quantile)]    \n",
    "    \n",
    "sd_columns = [col for col in train.columns if col.endswith('_sd')]\n",
    "train = train.drop(columns=sd_columns)\n",
    "    \n",
    "CONFIG.N_TRAIN_SAMPLES = len(train)\n",
    "CONFIG.N_STEPS_PER_EPOCH = (CONFIG.N_TRAIN_SAMPLES // CONFIG.BATCH_SIZE)\n",
    "CONFIG.N_STEPS = CONFIG.N_STEPS_PER_EPOCH * CONFIG.N_EPOCHS + 1    \n",
    "CONFIG.TABULAR_COLUMNS = train.filter(regex='^(WORLDCLIM_BIO|SOIL|MODIS_2000|VOD)').columns\n",
    "    \n",
    "if CONFIG.TRAIN_MODEL:\n",
    "    print('N_TRAIN_SAMPLES:', len(train), 'N_TEST_SAMPLES:', len(test))\n",
    "else:\n",
    "    print('N_TEST_SAMPLES:', len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd42d1b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T20:46:22.104860Z",
     "iopub.status.busy": "2024-04-20T20:46:22.104490Z",
     "iopub.status.idle": "2024-04-20T20:46:22.127080Z",
     "shell.execute_reply": "2024-04-20T20:46:22.126269Z"
    },
    "papermill": {
     "duration": 0.030286,
     "end_time": "2024-04-20T20:46:22.129323",
     "exception": false,
     "start_time": "2024-04-20T20:46:22.099037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if CONFIG.TRAIN_MODEL:\n",
    "LOG_FEATURES = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n",
    "\n",
    "y_df = np.zeros_like(train[CONFIG.TARGET_COLUMNS], dtype=np.float32)\n",
    "for target_idx, target in enumerate(CONFIG.TARGET_COLUMNS):\n",
    "    v = train[target].values\n",
    "    if target in LOG_FEATURES:\n",
    "        v = np.log10(v)\n",
    "    y_df[:, target_idx] = v\n",
    "\n",
    "SCALER = StandardScaler()\n",
    "y_df = SCALER.fit_transform(y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98d728f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T20:46:22.140205Z",
     "iopub.status.busy": "2024-04-20T20:46:22.139909Z",
     "iopub.status.idle": "2024-04-20T20:46:22.345036Z",
     "shell.execute_reply": "2024-04-20T20:46:22.343916Z"
    },
    "papermill": {
     "duration": 0.213731,
     "end_time": "2024-04-20T20:46:22.347818",
     "exception": false,
     "start_time": "2024-04-20T20:46:22.134087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SCALER_tabular = StandardScaler()\n",
    "tabular_train = SCALER_tabular.fit_transform(train[CONFIG.TABULAR_COLUMNS])\n",
    "tabular_test = SCALER_tabular.fit_transform(test[CONFIG.TABULAR_COLUMNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0b62c1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T20:46:22.359917Z",
     "iopub.status.busy": "2024-04-20T20:46:22.359541Z",
     "iopub.status.idle": "2024-04-20T20:46:22.376745Z",
     "shell.execute_reply": "2024-04-20T20:46:22.375757Z"
    },
    "papermill": {
     "duration": 0.025918,
     "end_time": "2024-04-20T20:46:22.379033",
     "exception": false,
     "start_time": "2024-04-20T20:46:22.353115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MEAN = np.array([0.485, 0.456, 0.406])\n",
    "STD = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "TRAIN_TRANSFORMS = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.4),\n",
    "        A.RandomSizedCrop(\n",
    "            [int(0.85*CONFIG.IMAGE_SIZE), CONFIG.IMAGE_SIZE],\n",
    "            CONFIG.IMAGE_SIZE, CONFIG.IMAGE_SIZE, w2h_ratio=1.0, p=0.75),\n",
    "        A.Resize(CONFIG.IMAGE_SIZE, CONFIG.IMAGE_SIZE),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.25),\n",
    "        A.HueSaturationValue(p=0.3),\n",
    "        A.ImageCompression(quality_lower=85, quality_upper=100, p=0.25),\n",
    "        A.ToFloat(),\n",
    "        A.Normalize(mean=MEAN, std=STD, max_pixel_value=1),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "TEST_TRANSFORMS = A.Compose([\n",
    "        A.Resize(CONFIG.IMAGE_SIZE, CONFIG.IMAGE_SIZE),\n",
    "        A.ToFloat(),\n",
    "        A.Normalize(mean=MEAN, std=STD, max_pixel_value=1),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, X_jpeg_bytes, X_tabular, y, transforms=None):\n",
    "        self.X_jpeg_bytes = X_jpeg_bytes\n",
    "        self.X_tabular = X_tabular\n",
    "        self.y = y\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_jpeg_bytes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X_sample = self.transforms(\n",
    "            image=imageio.imread(self.X_jpeg_bytes[index]),\n",
    "        )\n",
    "        X_sample['tabular'] = self.X_tabular[index].astype('float32')\n",
    "        y_sample = self.y[index]\n",
    "        \n",
    "        return X_sample, y_sample\n",
    "\n",
    "if CONFIG.TRAIN_MODEL:\n",
    "    # Creating datasets for training and validation\n",
    "    train_dataset = Dataset(\n",
    "        train['jpeg_bytes'].values,\n",
    "        tabular_train,\n",
    "        y_df,\n",
    "        TRAIN_TRANSFORMS,\n",
    "    )\n",
    "\n",
    "    # Creating dataloaders for training and validation\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CONFIG.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=psutil.cpu_count(),\n",
    "    )\n",
    "\n",
    "test_dataset = Dataset(\n",
    "    test['jpeg_bytes'].values,\n",
    "    tabular_test,\n",
    "    test['id'].values,\n",
    "    TEST_TRANSFORMS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "742b0815",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T20:46:22.390025Z",
     "iopub.status.busy": "2024-04-20T20:46:22.389689Z",
     "iopub.status.idle": "2024-04-20T20:46:43.737937Z",
     "shell.execute_reply": "2024-04-20T20:46:43.736871Z"
    },
    "papermill": {
     "duration": 21.356911,
     "end_time": "2024-04-20T20:46:43.740726",
     "exception": false,
     "start_time": "2024-04-20T20:46:22.383815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9350a4292744cba6baade333504f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/801M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "                CONFIG.BACKBONE,\n",
    "                pretrained=True,\n",
    "                num_classes=0,\n",
    "        )\n",
    "        \n",
    "        # EfficientNet = 1280, SwinTrans = 1536, Tabular = 163\n",
    "        self.custom_layers = nn.Sequential(\n",
    "            nn.Linear(1536+163, 512),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(512, 6)  \n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        image = inputs['image']\n",
    "        tabular = inputs['tabular']\n",
    "\n",
    "        x = self.backbone(image)\n",
    "        x = torch.cat((tabular, x), dim=1)\n",
    "        x = self.custom_layers(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "model = Model()\n",
    "model = model.to(device)\n",
    "# print(model.backbone.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30422fd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T20:46:43.752677Z",
     "iopub.status.busy": "2024-04-20T20:46:43.752031Z",
     "iopub.status.idle": "2024-04-20T20:46:43.772254Z",
     "shell.execute_reply": "2024-04-20T20:46:43.771390Z"
    },
    "papermill": {
     "duration": 0.028069,
     "end_time": "2024-04-20T20:46:43.774147",
     "exception": false,
     "start_time": "2024-04-20T20:46:43.746078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_lr_scheduler(optimizer):\n",
    "    return torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer=optimizer,\n",
    "        max_lr=CONFIG.LR_MAX,\n",
    "        total_steps=CONFIG.N_STEPS,\n",
    "        pct_start=0.1,\n",
    "        anneal_strategy='cos',\n",
    "        div_factor=1e1,\n",
    "        final_div_factor=1e1,\n",
    "    )\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        self.sum += val.sum()\n",
    "        self.count += val.numel()\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "if CONFIG.TRAIN_MODEL:\n",
    "    MAE = torchmetrics.regression.MeanAbsoluteError().to(device)\n",
    "    R2 = torchmetrics.regression.R2Score(num_outputs=CONFIG.N_TARGETS, multioutput='uniform_average').to(device)\n",
    "    LOSS = AverageMeter()\n",
    "\n",
    "    Y_MEAN = torch.tensor(y_df).mean(dim=0).to(device)\n",
    "    EPS = torch.tensor([1e-6]).to(device)\n",
    "\n",
    "    def r2_loss(y_pred, y_true):\n",
    "        ss_res = torch.sum((y_true - y_pred)**2, dim=0)\n",
    "        ss_total = torch.sum((y_true - Y_MEAN)**2, dim=0)\n",
    "        ss_total = torch.maximum(ss_total, EPS)\n",
    "        r2 = torch.mean(ss_res / ss_total)\n",
    "        return r2\n",
    "\n",
    "    LOSS_FN = nn.SmoothL1Loss() #r2_loss\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params=model.parameters(),\n",
    "        lr=CONFIG.LR_MAX,\n",
    "        weight_decay=CONFIG.WEIGHT_DECAY,\n",
    "    )\n",
    "\n",
    "    LR_SCHEDULER = get_lr_scheduler(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c09b300f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T20:46:43.785813Z",
     "iopub.status.busy": "2024-04-20T20:46:43.785217Z",
     "iopub.status.idle": "2024-04-21T08:39:43.816522Z",
     "shell.execute_reply": "2024-04-21T08:39:43.815424Z"
    },
    "papermill": {
     "duration": 42780.03972,
     "end_time": "2024-04-21T08:39:43.819120",
     "exception": false,
     "start_time": "2024-04-20T20:46:43.779400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 01, 4916/4916 | loss: 0.2673, mae: 0.5933, r2: 0.4029, step: 1.350s, lr: 9.94e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 02, 4916/4916 | loss: 0.2368, mae: 0.5488, r2: 0.4752, step: 1.231s, lr: 9.00e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 03, 4916/4916 | loss: 0.2067, mae: 0.5047, r2: 0.5469, step: 1.237s, lr: 7.09e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 04, 4916/4916 | loss: 0.1706, mae: 0.4497, r2: 0.6314, step: 1.232s, lr: 4.68e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 05, 4916/4916 | loss: 0.1347, mae: 0.3921, r2: 0.7134, step: 1.235s, lr: 2.36e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 06, 4916/4916 | loss: 0.1059, mae: 0.3432, r2: 0.7776, step: 1.236s, lr: 7.03e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 07, 4916/4916 | loss: 0.0904, mae: 0.3152, r2: 0.8118, step: 1.230s, lr: 1.00e-06\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.TRAIN_MODEL:\n",
    "    print(\"Start Training:\")\n",
    "    \n",
    "    best = float('inf')\n",
    "    for epoch in range(CONFIG.N_EPOCHS):\n",
    "        MAE.reset()\n",
    "        R2.reset()\n",
    "        LOSS.reset()\n",
    "        model.train()\n",
    "\n",
    "        for step, (X_batch, y_true) in enumerate(train_dataloader):\n",
    "            X_batch['image'] = X_batch['image'].to(device)\n",
    "            X_batch['tabular'] = X_batch['tabular'].to(device)\n",
    "            y_true = y_true.to(device)\n",
    "            t_start = time.perf_counter_ns()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = LOSS_FN(y_pred, y_true)\n",
    "            LOSS.update(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            LR_SCHEDULER.step()\n",
    "            MAE.update(y_pred, y_true)\n",
    "            R2.update(y_pred, y_true)\n",
    "\n",
    "            if not CONFIG.IS_INTERACTIVE and (step+1) == len(train_dataloader):\n",
    "                print(\n",
    "                    f'EPOCH {epoch+1:02d}, {step+1:04d}/{len(train_dataloader)} | ' + \n",
    "                    f'loss: {LOSS.avg:.4f}, mae: {MAE.compute().item():.4f}, r2: {R2.compute().item():.4f}, ' +\n",
    "                    f'step: {(time.perf_counter_ns()-t_start)*1e-9:.3f}s, lr: {LR_SCHEDULER.get_last_lr()[0]:.2e}',\n",
    "                )\n",
    "            elif CONFIG.IS_INTERACTIVE:\n",
    "                print(\n",
    "                    f'\\rEPOCH {epoch+1:02d}, {step+1:04d}/{len(train_dataloader)} | ' + \n",
    "                    f'loss: {LOSS.avg:.4f}, mae: {MAE.compute().item():.4f}, r2: {R2.compute().item():.4f}, ' +\n",
    "                    f'step: {(time.perf_counter_ns()-t_start)*1e-9:.3f}s, lr: {LR_SCHEDULER.get_last_lr()[0]:.2e}',\n",
    "                    end='\\n' if (step + 1) == CONFIG.N_STEPS_PER_EPOCH else '', flush=True,\n",
    "                )\n",
    "            \n",
    "    torch.save(model, 'model.pth')    \n",
    "else:\n",
    "    model = torch.load(CONFIG.MODEL_PATH)\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e2a070c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T08:39:43.832749Z",
     "iopub.status.busy": "2024-04-21T08:39:43.832361Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2024-04-21T08:39:43.825148",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c5656cbd5e4209bad671862dde502a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6545 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SUBMISSION_ROWS = []\n",
    "model.eval()\n",
    "\n",
    "for X_sample_test, test_id in tqdm(test_dataset):\n",
    "    X_sample_test['image'] = torch.Tensor(X_sample_test['image']).unsqueeze(0).to(device)\n",
    "    X_sample_test['tabular'] = torch.Tensor(X_sample_test['tabular']).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_sample_test).detach().cpu().numpy()\n",
    "    \n",
    "    y_pred = SCALER.inverse_transform(y_pred).squeeze()\n",
    "    row = {'id': test_id}\n",
    "    \n",
    "    for k, v in zip(CONFIG.TARGET_COLUMNS, y_pred):\n",
    "        if k in LOG_FEATURES:\n",
    "            row[k.replace('_mean', '')] = 10 ** v\n",
    "        else:\n",
    "            row[k.replace('_mean', '')] = v\n",
    "\n",
    "    SUBMISSION_ROWS.append(row)\n",
    "    \n",
    "submission_df = pd.DataFrame(SUBMISSION_ROWS)\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Submit!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8046133,
     "sourceId": 65626,
     "sourceType": "competition"
    },
    {
     "datasetId": 4761376,
     "sourceId": 8069756,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4840600,
     "sourceId": 8177321,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-20T20:45:39.538772",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}