{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f7b4cd",
   "metadata": {
    "papermill": {
     "duration": 0.014811,
     "end_time": "2024-03-12T09:01:44.107906",
     "exception": false,
     "start_time": "2024-03-12T09:01:44.093095",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Descripion\n",
    "\n",
    "## The notebook is based on the excellent version of the [HMS Resnet1D-GRU Train notebook by Med Ali Bouchhioua](https://www.kaggle.com/code/medali1992/hms-resnet1d-gru-train?scriptVersionId=163575181)\n",
    "\n",
    "## Changes 1 [LB:0.40]:\n",
    "\n",
    "- Convolution kernel used [3,5,7,9,11]\n",
    "- Loss functions replaced with Hardswish and SiLU\n",
    "- Adan optimizer replaced with AdamW\n",
    "- Bandpass filter with a lower limit of 0.5 Hz\n",
    "- Total Evaluators are used in the first data set from 0 to 5, the second data set from 6 to max\n",
    "- Albumentations. Random frequency cut with a bandpass filter in the range 10 - 25 Hz\n",
    "- 20 epochs with two stages\n",
    "\n",
    "## Changes 2 [LB:0.38]:\n",
    "- Order of filter changed from 6 to 2 and high cutoff frequency changed from 25 Hz to 20 Hz. An order with order 6 has a very strong effect on the signal if there are sharp jumps.\n",
    "\n",
    "## Changes 3 [LB:0.38]:\n",
    "- The total of appraisers is divided into three parts: [0..2], [3..5], [6..1000].\n",
    "\n",
    "## Changes 4 [LB:0.40]:\n",
    "- Return to total of appraisers is divided into two parts: [0..5], [6..1000].\n",
    "- Remove Regularization value 0.166666667 according to the advice of [Med Ali Bouchhioua](https://www.kaggle.com/code/konstantinboyko/hms-resnet1d-gru-v22-human-6-train/comments#2681934).\n",
    "- Added code that allows you to train the model not only in the stage/fold section, but also in the reverse fold/stage section.\n",
    "- Added filter parameters.\n",
    "\n",
    "## Changes 5 [LB:0.39]:\n",
    "- Albumentations. Accidentally missing an entire signal.\n",
    "\n",
    "## Changes 6 [LB:0.38]\n",
    "- The signal size is reduced by half and is selected randomly from the total signal.\n",
    "\n",
    "## Changes 7 [LB:0.36]\n",
    "- The signal size is reduced by five times and selected randomly from the total signal.\n",
    "\n",
    "## Changes 8 [LB:0.39]\n",
    "- One-stage model with the number of evaluators in the range [5..Max]\n",
    "\n",
    "## Changes 9 [LB:0.38]\n",
    "- The signal size is reduced by five times and selected randomly from the total signal.\n",
    "- Total Evaluators in the range [2..2 + 6..28]\n",
    "\n",
    "## Changes 10 [LB:0.38]\n",
    "- Random general reversal of the signal\n",
    "- Random general signal flipping upside down\n",
    "\n",
    "## Changes 11 [LB:0.37]\n",
    "- The total of appraisers is divided into two parts: [1..2 + 4..5], [6..28].\n",
    "\n",
    "## Changes 12 [LB:0.36]\n",
    "- The total of appraisers is divided into two parts: [1..5 -4(GPD)], [6..28]\n",
    "\n",
    "## Changes 13 [LB:]\n",
    "- I further trained the model in the range of estimators [10..28]\n",
    "\n",
    "## [Final Dataset](https://www.kaggle.com/datasets/konstantinboyko/hms-resnet1d-gru-weights-v90)\n",
    "\n",
    "## [Previous Train](https://www.kaggle.com/code/konstantinboyko/hms-resnet1d-gru-v33-human-5-stage-1-train)\n",
    "\n",
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3925269a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:01:44.137422Z",
     "iopub.status.busy": "2024-03-12T09:01:44.137109Z",
     "iopub.status.idle": "2024-03-12T09:01:52.262164Z",
     "shell.execute_reply": "2024-03-12T09:01:52.260785Z"
    },
    "papermill": {
     "duration": 8.142307,
     "end_time": "2024-03-12T09:01:52.264416",
     "exception": false,
     "start_time": "2024-03-12T09:01:44.122109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ubuntu 20.04.6 LTS\r\n",
      "BUILD_DATE=20240222-122512, CONTAINER_NAME=tf2-gpu/2-15+cu121\n",
      "PyTorch Version:2.1.2, CUDA is available:True, Version CUDA:12.1\n",
      "Device Capability:(6, 0), ['sm_60', 'sm_70', 'sm_75', 'compute_70', 'compute_75']\n",
      "CuDNN Enabled:True, Version:8900\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Union\n",
    "import scipy.signal as scisig\n",
    "from scipy.signal import butter, lfilter, freqz\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import (\n",
    "    ReduceLROnPlateau,\n",
    "    OneCycleLR,\n",
    "    CosineAnnealingLR,\n",
    "    CosineAnnealingWarmRestarts,\n",
    ")\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "#import cupy as cp\n",
    "#import cupyx.scipy.signal as cpsig\n",
    "\n",
    "sys.path.append(\"/kaggle/input/kaggle-kl-div\")\n",
    "import kaggle_kl_div\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "!cat /etc/os-release | grep -oP \"PRETTY_NAME=\\\"\\K([^\\\"]*)\"\n",
    "print(f\"BUILD_DATE={os.environ['BUILD_DATE']}, CONTAINER_NAME={os.environ['CONTAINER_NAME']}\")\n",
    "\n",
    "try:\n",
    "    print(\n",
    "        f\"PyTorch Version:{torch.__version__}, CUDA is available:{torch.cuda.is_available()}, Version CUDA:{torch.version.cuda}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Device Capability:{torch.cuda.get_device_capability()}, {torch.cuda.get_arch_list()}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"CuDNN Enabled:{torch.backends.cudnn.enabled}, Version:{torch.backends.cudnn.version()}\"\n",
    "    )\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeb71f0",
   "metadata": {
    "papermill": {
     "duration": 0.014798,
     "end_time": "2024-03-12T09:01:52.294028",
     "exception": false,
     "start_time": "2024-03-12T09:01:52.279230",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Directory settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b581a4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:01:52.326702Z",
     "iopub.status.busy": "2024-03-12T09:01:52.325361Z",
     "iopub.status.idle": "2024-03-12T09:01:52.336835Z",
     "shell.execute_reply": "2024-03-12T09:01:52.335945Z"
    },
    "papermill": {
     "duration": 0.030106,
     "end_time": "2024-03-12T09:01:52.338837",
     "exception": false,
     "start_time": "2024-03-12T09:01:52.308731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jupyter:True, kaggle:True, local:False\n",
      ".\n",
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "class APP:\n",
    "    jupyter = \"ipykernel\" in globals()\n",
    "    if not jupyter:\n",
    "        try:\n",
    "            if \"IPython\" in globals().get(\"__doc__\", \"\"):\n",
    "                jupyter = True\n",
    "        except Exception as inst:\n",
    "            print(inst)\n",
    "\n",
    "    kaggle = os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\", \"\") != \"\"\n",
    "    local = os.environ.get(\"DOCKER_USING\", \"\") == \"LOCAL\"\n",
    "    date_time_start = dt.datetime.now()\n",
    "    dt_start_ymd_hms = date_time_start.strftime(\"%Y.%m.%d_%H-%M-%S\")\n",
    "\n",
    "    file_run_path = \"\"\n",
    "    if jupyter:\n",
    "        try:\n",
    "            file_run_path = Path(globals().get(\"__vsc_ipynb_file__\", \"\"))\n",
    "        except Exception as inst:\n",
    "            print(inst)\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            file_run_path = Path(__file__)\n",
    "        except Exception as inst:\n",
    "            print(inst)\n",
    "\n",
    "    file_run_name = file_run_path.stem\n",
    "    path_app = file_run_path.parent\n",
    "    path_run = Path(os.getcwd())\n",
    "    path_out = (\n",
    "        Path(\"/kaggle/working\")\n",
    "        if kaggle\n",
    "        else file_run_path / f\"{file_run_name}_{dt_start_ymd_hms}\"\n",
    "    )\n",
    "\n",
    "\n",
    "OUTPUT_DIR = \"./\"\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "print(f\"jupyter:{APP.jupyter}, kaggle:{APP.kaggle}, local:{APP.local}\")\n",
    "print(APP.file_run_path)\n",
    "print(APP.path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f89b92",
   "metadata": {
    "papermill": {
     "duration": 0.014367,
     "end_time": "2024-03-12T09:01:52.367577",
     "exception": false,
     "start_time": "2024-03-12T09:01:52.353210",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca7d11e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:01:52.398180Z",
     "iopub.status.busy": "2024-03-12T09:01:52.397888Z",
     "iopub.status.idle": "2024-03-12T09:01:52.426247Z",
     "shell.execute_reply": "2024-03-12T09:01:52.425248Z"
    },
    "papermill": {
     "duration": 0.0461,
     "end_time": "2024-03-12T09:01:52.428176",
     "exception": false,
     "start_time": "2024-03-12T09:01:52.382076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Fp1': 0, 'T3': 1, 'C3': 2, 'O1': 3, 'Fp2': 4, 'C4': 5, 'T4': 6, 'O2': 7}\n",
      "['Fp1', 'T3', 'C3', 'O1', 'Fp2', 'C4', 'T4', 'O2']\n"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    "    VERSION = 64  #'90'\n",
    "\n",
    "    wandb = False\n",
    "    debug = False\n",
    "    create_eegs = False\n",
    "    apex = True\n",
    "    visualize = False\n",
    "    save_all_models = True\n",
    "\n",
    "    if debug:\n",
    "        num_workers = 0\n",
    "        parallel = False\n",
    "    else:\n",
    "        num_workers = os.cpu_count()\n",
    "        parallel = True\n",
    "\n",
    "    model_name = \"resnet1d_gru\"\n",
    "    # optimizer = \"Adan\"\n",
    "    optimizer = \"AdamW\"\n",
    "\n",
    "    factor = 0.9\n",
    "    eps = 1e-6\n",
    "    lr = 8e-3\n",
    "    min_lr = 1e-6\n",
    "\n",
    "    batch_size = 64\n",
    "    batch_koef_valid = 2\n",
    "    batch_scheduler = True\n",
    "    weight_decay = 1e-2\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 1e7\n",
    "\n",
    "    fixed_kernel_size = 5\n",
    "    # linear_layer_features = 424\n",
    "    # kernels = [3, 5, 7, 9]\n",
    "    #linear_layer_features = 448  # Full Signal = 10_000\n",
    "    #linear_layer_features = 352  # Half Signal = 5_000\n",
    "    linear_layer_features = 304   # 1/4, 1/5, 1/6  Signal = 2_000\n",
    "    #linear_layer_features = 280  # 1/10  Signal = 1_000\n",
    "    kernels = [3, 5, 7, 9, 11]\n",
    "    # kernels = [5, 7, 9, 11, 13]\n",
    "\n",
    "    seq_length = 50  # Second's\n",
    "    sampling_rate = 200  # Hz\n",
    "    nsamples = seq_length * sampling_rate  # Число семплов 10_000\n",
    "    n_split_samples = 5\n",
    "    out_samples = nsamples // n_split_samples  # 2_000\n",
    "    sample_delta = nsamples - out_samples  # 8000\n",
    "    sample_offset = sample_delta // 2\n",
    "    multi_validation = False\n",
    "\n",
    "    train_by_stages = False\n",
    "    train_by_folds = True\n",
    "\n",
    "    # 'GPD', 'GRDA', 'LPD', 'LRDA', 'Other', 'Seizure'\n",
    "    n_stages = 4\n",
    "    match n_stages:\n",
    "        case 1:\n",
    "            train_stages = [0]\n",
    "            epochs = [100]\n",
    "            test_total_eval = 2\n",
    "            total_evals_old = [[(2, 3), (6, 29)]]  # Deprecated\n",
    "            total_evaluators = [ \n",
    "                [   \n",
    "                    {'band':(2, 2), 'excl_evals':[]}, \n",
    "                    {'band':(6, 28), 'excl_evals':[]},\n",
    "                ], \n",
    "            ]            \n",
    "        case 2:\n",
    "            train_stages = [0, 1]\n",
    "            epochs = [20, 100]\n",
    "            test_total_eval = 0\n",
    "            total_evals_old = [[(1, 2),(4, 5)], (6, 29)]  # Deprecated\n",
    "            total_evaluators = [ \n",
    "                [   \n",
    "                    {'band':(1, 5), 'excl_evals':[]},\n",
    "                ], \n",
    "                [   \n",
    "                    {'band':(6, 28), 'excl_evals':[]}, \n",
    "                ], \n",
    "            ]            \n",
    "        case 3:\n",
    "            train_stages = [0, 1, 2]\n",
    "#             train_stages = [2]\n",
    "            epochs = [50, 100, 10]\n",
    "            test_total_eval = 0\n",
    "            total_evals_old = [(0, 3), (3, 6), (6, 29)]  # Deprecated\n",
    "            total_evaluators = [ \n",
    "                [   \n",
    "                    {'band':(0, 5), 'excl_evals':[]}, \n",
    "                ], \n",
    "                [   \n",
    "                    {'band':(6, 28), 'excl_evals':[]}, \n",
    "                ], \n",
    "                [   \n",
    "                    {'band':(10, 28), 'excl_evals':[]},\n",
    "                ], \n",
    "            ]    \n",
    "        case 4: # Added this to create the same training procedure as the EfficientNet model\n",
    "            train_stages = [0, 1]\n",
    "            epochs = [50, 50]\n",
    "            test_total_eval = 0\n",
    "            total_evaluators = [ \n",
    "                [   \n",
    "                    {'band':(0, 28), 'excl_evals':[]}, \n",
    "                ], \n",
    "                [   \n",
    "                    {'band':(10, 28), 'excl_evals':[]}, \n",
    "                ]\n",
    "            ]            \n",
    "    \n",
    "    \n",
    "    n_fold = 5\n",
    "    train_folds = [0, 1, 2, 3, 4]\n",
    "    # train_folds = [0]\n",
    "\n",
    "    patience = 11\n",
    "    seed = 2024\n",
    "\n",
    "    bandpass_filter = {\"low\": 0.5, \"high\": 20, \"order\": 2}\n",
    "    rand_filter = {\"probab\": 0.1, \"low\": 10, \"high\": 20, \"band\": 1.0, \"order\": 2}\n",
    "    freq_channels = []  # [(8.0, 12.0)]; [(0.5, 4.5)]\n",
    "    filter_order = 2\n",
    "\n",
    "    random_divide_signal = 0.0\n",
    "    random_close_zone = 0.0\n",
    "    random_negative_signal = 0.0\n",
    "    random_reverse_signal = 0.0\n",
    "    random_common_negative_signal = 0.0\n",
    "    random_common_reverse_signal = 0.0\n",
    "\n",
    "    log_step = 100  # Шаг отображения тренировки\n",
    "    log_show = False\n",
    "\n",
    "    scheduler = \"CosineAnnealingWarmRestarts\"  # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts','OneCycleLR']\n",
    "\n",
    "    # CosineAnnealingLR params\n",
    "    cosanneal_params = {\n",
    "        \"T_max\": 6,\n",
    "        \"eta_min\": 1e-5,\n",
    "        \"last_epoch\": -1,\n",
    "    }\n",
    "\n",
    "    # ReduceLROnPlateau params\n",
    "    reduce_params = {\n",
    "        \"mode\": \"min\",\n",
    "        \"factor\": 0.2,\n",
    "        \"patience\": 4,\n",
    "        \"eps\": 1e-6,\n",
    "        \"verbose\": True,\n",
    "    }\n",
    "\n",
    "    # CosineAnnealingWarmRestarts params\n",
    "    cosanneal_res_params = {\n",
    "        \"T_0\": 20,\n",
    "        \"eta_min\": 1e-6,\n",
    "        \"T_mult\": 1,\n",
    "        \"last_epoch\": -1,\n",
    "    }\n",
    "\n",
    "    target_cols = [\n",
    "        \"seizure_vote\",\n",
    "        \"lpd_vote\",\n",
    "        \"gpd_vote\",\n",
    "        \"lrda_vote\",\n",
    "        \"grda_vote\",\n",
    "        \"other_vote\",\n",
    "    ]\n",
    "\n",
    "    pred_cols = [x + \"_pred\" for x in target_cols]\n",
    "\n",
    "    map_features = [\n",
    "        (\"Fp1\", \"T3\"),\n",
    "        (\"T3\", \"O1\"),\n",
    "        (\"Fp1\", \"C3\"),\n",
    "        (\"C3\", \"O1\"),\n",
    "        (\"Fp2\", \"C4\"),\n",
    "        (\"C4\", \"O2\"),\n",
    "        (\"Fp2\", \"T4\"),\n",
    "        (\"T4\", \"O2\"),\n",
    "        #('Fz', 'Cz'), ('Cz', 'Pz'),\n",
    "    ]\n",
    "\n",
    "    eeg_features = [\"Fp1\", \"T3\", \"C3\", \"O1\", \"Fp2\", \"C4\", \"T4\", \"O2\"]  # 'Fz', 'Cz', 'Pz'\n",
    "        # 'F3', 'P3', 'F7', 'T5', 'Fz', 'Cz', 'Pz', 'F4', 'P4', 'F8', 'T6', 'EKG']\n",
    "    feature_to_index = {x: y for x, y in zip(eeg_features, range(len(eeg_features)))}\n",
    "    simple_features = []  # 'Fz', 'Cz', 'Pz', 'EKG'\n",
    "\n",
    "    # eeg_features = [row for row in feature_to_index]\n",
    "    # eeg_feat_size = len(eeg_features)\n",
    "    \n",
    "    n_map_features = len(map_features)\n",
    "    in_channels = n_map_features + n_map_features * len(freq_channels) + len(simple_features)\n",
    "    target_size = len(target_cols)\n",
    "\n",
    "    path_inp = Path(\"/kaggle/input\")\n",
    "    path_src = path_inp / \"hms-harmful-brain-activity-classification/\"\n",
    "    file_train = path_src / \"train.csv\"\n",
    "    path_train = path_src / \"train_eegs\"\n",
    "    file_features_test = path_train / \"100261680.parquet\"\n",
    "    file_eeg_specs = path_inp / \"eeg-spectrogram-by-lead-id-unique/eeg_specs.npy\"\n",
    "    file_raw_eeg = path_inp / \"brain-eegs/eegs.npy\"\n",
    "    #file_raw_eeg = path_inp / \"brain-eegs-plus/eegs.npy\"\n",
    "    #file_raw_eeg = path_inp / \"brain-eegs-full/eegs.npy\"\n",
    "\n",
    "    if APP.kaggle:\n",
    "        num_workers = 2\n",
    "        parallel = True\n",
    "        # GPU_DEVICES = \"auto\"\n",
    "\n",
    "\n",
    "# print(CFG.eeg_feat_size, CFG.in_channels)\n",
    "print(CFG.feature_to_index)\n",
    "print(CFG.eeg_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d95f8b",
   "metadata": {
    "papermill": {
     "duration": 0.014141,
     "end_time": "2024-03-12T09:01:52.457036",
     "exception": false,
     "start_time": "2024-03-12T09:01:52.442895",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f2627c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:01:52.487095Z",
     "iopub.status.busy": "2024-03-12T09:01:52.486774Z",
     "iopub.status.idle": "2024-03-12T09:01:52.503632Z",
     "shell.execute_reply": "2024-03-12T09:01:52.502621Z"
    },
    "papermill": {
     "duration": 0.034502,
     "end_time": "2024-03-12T09:01:52.505892",
     "exception": false,
     "start_time": "2024-03-12T09:01:52.471390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_logger(log_file=OUTPUT_DIR + \"train.log\"):\n",
    "    from logging import getLogger, INFO, FileHandler, Formatter, StreamHandler\n",
    "\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def quantize_data(data, classes):\n",
    "    mu_x = mu_law_encoding(data, classes)\n",
    "    return mu_x  # quantized\n",
    "\n",
    "\n",
    "def mu_law_encoding(data, mu):\n",
    "    mu_x = np.sign(data) * np.log(1 + mu * np.abs(data)) / np.log(mu + 1)\n",
    "    return mu_x\n",
    "\n",
    "\n",
    "def mu_law_expansion(data, mu):\n",
    "    s = np.sign(data) * (np.exp(np.abs(data) * np.log(mu + 1)) - 1) / mu\n",
    "    return s\n",
    "\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    return butter(order, [lowcut, highcut], fs=fs, btype=\"band\")\n",
    "\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "def butter_lowpass_filter(\n",
    "    data, cutoff_freq=20, sampling_rate=CFG.sampling_rate, order=4\n",
    "):\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    normal_cutoff = cutoff_freq / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype=\"low\", analog=False)\n",
    "    filtered_data = lfilter(b, a, data, axis=0)\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def denoise_filter(x):\n",
    "    # Частота дискретизации и желаемые частоты среза (в Гц).\n",
    "    # Отфильтруйте шумный сигнал\n",
    "    y = butter_bandpass_filter(x, CFG.lowcut, CFG.highcut, CFG.sampling_rate, order=6)\n",
    "    y = (y + np.roll(y, -1) + np.roll(y, -2) + np.roll(y, -3)) / 4\n",
    "    y = y[0:-1:4]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f4a76a",
   "metadata": {
    "papermill": {
     "duration": 0.014333,
     "end_time": "2024-03-12T09:01:52.534525",
     "exception": false,
     "start_time": "2024-03-12T09:01:52.520192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Parquet to EEG Signals Numpy Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a4a8d2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:01:52.565077Z",
     "iopub.status.busy": "2024-03-12T09:01:52.564738Z",
     "iopub.status.idle": "2024-03-12T09:01:52.576526Z",
     "shell.execute_reply": "2024-03-12T09:01:52.575571Z"
    },
    "papermill": {
     "duration": 0.029476,
     "end_time": "2024-03-12T09:01:52.578489",
     "exception": false,
     "start_time": "2024-03-12T09:01:52.549013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eeg_from_parquet(\n",
    "    parquet_path: str, display: bool = False, seq_length=CFG.seq_length\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Эта функция читает файл паркета и извлекает средние 50 секунд показаний. Затем он заполняет значения NaN\n",
    "    со средним значением (игнорируя NaN).\n",
    "        :param parquet_path: путь к файлу паркета.\n",
    "        :param display: отображать графики ЭЭГ или нет.\n",
    "        :return data: np.array формы (time_steps, eeg_features) -> (10_000, 8)\n",
    "    \"\"\"\n",
    "\n",
    "    # Вырезаем среднюю 50 секундную часть\n",
    "    eeg = pd.read_parquet(parquet_path, columns=CFG.eeg_features)\n",
    "    rows = len(eeg)\n",
    "\n",
    "    # начало смещения данных, чтобы забрать середину\n",
    "    offset = (rows - CFG.nsamples) // 2\n",
    "\n",
    "    # средние 50 секунд, имеет одинаковое количество показаний слева и справа\n",
    "    eeg = eeg.iloc[offset : offset + CFG.nsamples]\n",
    "\n",
    "    if display:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        offset = 0\n",
    "\n",
    "    # Конвертировать в numpy\n",
    "\n",
    "    # создать заполнитель той же формы с нулями\n",
    "    data = np.zeros((CFG.nsamples, len(CFG.eeg_features)))\n",
    "\n",
    "    for index, feature in enumerate(CFG.eeg_features):\n",
    "        x = eeg[feature].values.astype(\"float32\")  # конвертировать в float32\n",
    "\n",
    "        # Вычисляет среднее арифметическое вдоль указанной оси, игнорируя NaN.\n",
    "        mean = np.nanmean(x)\n",
    "        nan_percentage = np.isnan(x).mean()  # percentage of NaN values in feature\n",
    "\n",
    "        # Заполнение значения Nan\n",
    "        # Поэлементная проверка на NaN и возврат результата в виде логического массива.\n",
    "        if nan_percentage < 1:  # если некоторые значения равны Nan, но не все\n",
    "            x = np.nan_to_num(x, nan=mean)\n",
    "        else:  # если все значения — Nan\n",
    "            x[:] = 0\n",
    "        data[:, index] = x\n",
    "\n",
    "        if display:\n",
    "            if index != 0:\n",
    "                offset += x.max()\n",
    "            plt.plot(range(CFG.nsamples), x - offset, label=feature)\n",
    "            offset -= x.min()\n",
    "\n",
    "    if display:\n",
    "        plt.legend()\n",
    "        name = parquet_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        plt.yticks([])\n",
    "        plt.title(f\"EEG {name}\", size=16)\n",
    "        plt.show()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fedfac4",
   "metadata": {
    "papermill": {
     "duration": 0.014118,
     "end_time": "2024-03-12T09:01:52.607070",
     "exception": false,
     "start_time": "2024-03-12T09:01:52.592952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d225685",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:01:52.637154Z",
     "iopub.status.busy": "2024-03-12T09:01:52.636899Z",
     "iopub.status.idle": "2024-03-12T09:01:52.666340Z",
     "shell.execute_reply": "2024-03-12T09:01:52.665556Z"
    },
    "papermill": {
     "duration": 0.046818,
     "end_time": "2024-03-12T09:01:52.668284",
     "exception": false,
     "start_time": "2024-03-12T09:01:52.621466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EEGDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        batch_size: int,\n",
    "        eegs: Dict[int, np.ndarray],\n",
    "        mode: str = \"train\",\n",
    "        downsample: int = None,\n",
    "        bandpass_filter: Dict[str, Union[int, float]] = None,\n",
    "        rand_filter: Dict[str, Union[int, float]] = None,\n",
    "    ):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.mode = mode\n",
    "        self.eegs = eegs\n",
    "        self.downsample = downsample\n",
    "        self.offset = None\n",
    "        self.bandpass_filter = bandpass_filter\n",
    "        self.rand_filter = rand_filter\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Length of dataset.\n",
    "        \"\"\"\n",
    "        # Обозначает количество пакетов за эпоху\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get one item.\n",
    "        \"\"\"\n",
    "        # Сгенерировать один пакет данных\n",
    "        X, y_prob = self.__data_generation(index)\n",
    "        if self.downsample is not None:\n",
    "            X = X[:: self.downsample, :]\n",
    "        output = {\n",
    "            \"eeg\": torch.tensor(X, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(y_prob, dtype=torch.float32),\n",
    "        }\n",
    "        return output\n",
    "\n",
    "    def set_offset(self, offset: int):\n",
    "        self.offset = offset\n",
    "\n",
    "    def __data_generation(self, index):\n",
    "        # Генерирует данные, содержащие образцы размера партии\n",
    "        X = np.zeros(\n",
    "            (CFG.out_samples, CFG.in_channels), dtype=\"float32\"\n",
    "        )  # Size=(10000, 14)\n",
    "\n",
    "        random_divide_signal = False\n",
    "        row = self.df.iloc[index]  # Строка Pandas\n",
    "        data = self.eegs[row.eeg_id]  # Size=(10000, 8)\n",
    "        if CFG.nsamples != CFG.out_samples:\n",
    "            if self.mode == \"train\":\n",
    "                offset = (CFG.sample_delta * random.randint(0, 1000)) // 1000\n",
    "            elif not self.offset is None:\n",
    "                offset = self.offset\n",
    "            else:\n",
    "                offset = CFG.sample_offset\n",
    "\n",
    "            if self.mode == \"train\" and CFG.random_divide_signal > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_divide_signal:\n",
    "                random_divide_signal = True\n",
    "                multipliers = [(1, 2), (2, 3), (3, 4), (3, 5)]\n",
    "                koef_1, koef_2 = multipliers[random.randint(0, 3)]\n",
    "                offset = (koef_1 * offset) // koef_2\n",
    "                data = data[offset:offset+(CFG.out_samples * koef_2) // koef_1,:]\n",
    "            else:\n",
    "                data = data[offset:offset+CFG.out_samples,:]\n",
    "\n",
    "        reverse_signal = False\n",
    "        negative_signal = False\n",
    "        if self.mode == \"train\":\n",
    "            if CFG.random_common_reverse_signal > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_common_reverse_signal:\n",
    "                reverse_signal = True\n",
    "            if CFG.random_common_negative_signal > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_common_negative_signal:\n",
    "                negative_signal = True\n",
    "\n",
    "        for i, (feat_a, feat_b) in enumerate(CFG.map_features):\n",
    "            if self.mode == \"train\" and CFG.random_close_zone > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_close_zone:\n",
    "                continue\n",
    "            \n",
    "            diff_feat = (\n",
    "                data[:, CFG.feature_to_index[feat_a]]\n",
    "                - data[:, CFG.feature_to_index[feat_b]]\n",
    "            )  # Size=(10000,)\n",
    "\n",
    "            if self.mode == \"train\":\n",
    "                if reverse_signal or CFG.random_reverse_signal > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_reverse_signal:\n",
    "                    diff_feat = np.flip(diff_feat)\n",
    "                if negative_signal or CFG.random_negative_signal > 0.0 and random.uniform(0.0, 1.0) <= CFG.random_negative_signal:\n",
    "                    diff_feat = -diff_feat\n",
    "\n",
    "            if not self.bandpass_filter is None:\n",
    "                diff_feat = butter_bandpass_filter(\n",
    "                    diff_feat,\n",
    "                    self.bandpass_filter[\"low\"],\n",
    "                    self.bandpass_filter[\"high\"],\n",
    "                    CFG.sampling_rate,\n",
    "                    order=self.bandpass_filter[\"order\"],\n",
    "                )\n",
    "            \n",
    "            if random_divide_signal:\n",
    "                #diff_feat = cp.asnumpy(cpsig.upfirdn([1.0, 1, 1.0], diff_feat, 2, 3))  # linear interp, rate 2/3\n",
    "                diff_feat = scisig.upfirdn([1.0, 1, 1.0], diff_feat, koef_1, koef_2)  # linear interp, rate 2/3\n",
    "                diff_feat = diff_feat[0:CFG.out_samples]\n",
    "\n",
    "            if (\n",
    "                self.mode == \"train\"\n",
    "                and not self.rand_filter is None\n",
    "                and random.uniform(0.0, 1.0) <= self.rand_filter[\"probab\"]\n",
    "            ):\n",
    "                lowcut = random.randint(\n",
    "                    self.rand_filter[\"low\"], self.rand_filter[\"high\"]\n",
    "                )\n",
    "                highcut = lowcut + self.rand_filter[\"band\"]\n",
    "                diff_feat = butter_bandpass_filter(\n",
    "                    diff_feat,\n",
    "                    lowcut,\n",
    "                    highcut,\n",
    "                    CFG.sampling_rate,\n",
    "                    order=self.rand_filter[\"order\"],\n",
    "                )\n",
    "\n",
    "            X[:, i] = diff_feat\n",
    "\n",
    "        n = CFG.n_map_features\n",
    "        if len(CFG.freq_channels) > 0:\n",
    "            for i in range(CFG.n_map_features):\n",
    "                diff_feat = X[:, i]\n",
    "                for j, (lowcut, highcut) in enumerate(CFG.freq_channels):\n",
    "                    band_feat = butter_bandpass_filter(\n",
    "                        diff_feat, lowcut, highcut, CFG.sampling_rate, order=CFG.filter_order,  # 6\n",
    "                    )\n",
    "                    X[:, n] = band_feat\n",
    "                    n += 1\n",
    "\n",
    "        for spml_feat in CFG.simple_features:\n",
    "            feat_val = data[:, CFG.feature_to_index[spml_feat]]\n",
    "            \n",
    "            if not self.bandpass_filter is None:\n",
    "                feat_val = butter_bandpass_filter(\n",
    "                    feat_val,\n",
    "                    self.bandpass_filter[\"low\"],\n",
    "                    self.bandpass_filter[\"high\"],\n",
    "                    CFG.sampling_rate,\n",
    "                    order=self.bandpass_filter[\"order\"],\n",
    "                )\n",
    "\n",
    "            if (\n",
    "                self.mode == \"train\"\n",
    "                and not self.rand_filter is None\n",
    "                and random.uniform(0.0, 1.0) <= self.rand_filter[\"probab\"]\n",
    "            ):\n",
    "                lowcut = random.randint(\n",
    "                    self.rand_filter[\"low\"], self.rand_filter[\"high\"]\n",
    "                )\n",
    "                highcut = lowcut + self.rand_filter[\"band\"]\n",
    "                feat_val = butter_bandpass_filter(\n",
    "                    feat_val,\n",
    "                    lowcut,\n",
    "                    highcut,\n",
    "                    CFG.sampling_rate,\n",
    "                    order=self.rand_filter[\"order\"],\n",
    "                )\n",
    "\n",
    "            X[:, n] = feat_val\n",
    "            n += 1\n",
    "            \n",
    "        # Обрезать края превышающие значения [-1024, 1024]\n",
    "        X = np.clip(X, -1024, 1024)\n",
    "\n",
    "        # Замените NaN нулем и разделить все на 32\n",
    "        X = np.nan_to_num(X, nan=0) / 32.0\n",
    "\n",
    "        # обрезать полосовым фильтром верхнюю границу в 20 Hz.\n",
    "        X = butter_lowpass_filter(X, order=CFG.filter_order)  # 4\n",
    "\n",
    "        y_prob = np.zeros(CFG.target_size, dtype=\"float32\")  # Size=(6,)\n",
    "        if self.mode != \"test\":\n",
    "            y_prob = row[CFG.target_cols].values.astype(np.float32)\n",
    "\n",
    "        return X, y_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afe0574",
   "metadata": {
    "papermill": {
     "duration": 0.014192,
     "end_time": "2024-03-12T09:01:52.696881",
     "exception": false,
     "start_time": "2024-03-12T09:01:52.682689",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63321aac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:01:52.726746Z",
     "iopub.status.busy": "2024-03-12T09:01:52.726210Z",
     "iopub.status.idle": "2024-03-12T09:01:52.734869Z",
     "shell.execute_reply": "2024-03-12T09:01:52.734058Z"
    },
    "papermill": {
     "duration": 0.025616,
     "end_time": "2024-03-12T09:01:52.736746",
     "exception": false,
     "start_time": "2024-03-12T09:01:52.711130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KLDivLossWithLogits(nn.KLDivLoss):\n",
    "    def __init__(self):\n",
    "        super().__init__(reduction=\"batchmean\")\n",
    "\n",
    "    def forward(self, y, t):\n",
    "        y = nn.functional.log_softmax(y, dim=1)\n",
    "        loss = super().forward(y, t)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = True  # Это опция требует много паямяти GPU\n",
    "    # pl.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd3f50",
   "metadata": {
    "papermill": {
     "duration": 0.014018,
     "end_time": "2024-03-12T09:01:52.765191",
     "exception": false,
     "start_time": "2024-03-12T09:01:52.751173",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1793ad5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:01:52.795352Z",
     "iopub.status.busy": "2024-03-12T09:01:52.795077Z",
     "iopub.status.idle": "2024-03-12T09:01:52.820370Z",
     "shell.execute_reply": "2024-03-12T09:01:52.819541Z"
    },
    "papermill": {
     "duration": 0.042781,
     "end_time": "2024-03-12T09:01:52.822384",
     "exception": false,
     "start_time": "2024-03-12T09:01:52.779603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResNet_1D_Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        padding,\n",
    "        downsampling,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        super(ResNet_1D_Block, self).__init__()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=in_channels)\n",
    "        # self.relu = nn.ReLU(inplace=False)\n",
    "        # self.relu_1 = nn.PReLU()\n",
    "        # self.relu_2 = nn.PReLU()\n",
    "        self.relu_1 = nn.Hardswish()\n",
    "        self.relu_2 = nn.Hardswish()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout, inplace=False)\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=out_channels)\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.maxpool = nn.MaxPool1d(\n",
    "            kernel_size=2,\n",
    "            stride=2,\n",
    "            padding=0,\n",
    "            dilation=dilation,\n",
    "        )\n",
    "        self.downsampling = downsampling\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu_1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu_2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        out = self.maxpool(out)\n",
    "        identity = self.downsampling(x)\n",
    "\n",
    "        out += identity\n",
    "        return out\n",
    "\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernels,\n",
    "        in_channels,\n",
    "        fixed_kernel_size,\n",
    "        num_classes,\n",
    "        linear_layer_features,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "    ):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.kernels = kernels\n",
    "        self.planes = 24\n",
    "        self.parallel_conv = nn.ModuleList()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        for i, kernel_size in enumerate(list(self.kernels)):\n",
    "            sep_conv = nn.Conv1d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=self.planes,\n",
    "                kernel_size=(kernel_size),\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                dilation=dilation,\n",
    "                groups=groups,\n",
    "                bias=False,\n",
    "            )\n",
    "            self.parallel_conv.append(sep_conv)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=self.planes)\n",
    "        # self.relu = nn.ReLU(inplace=False)\n",
    "        # self.relu_1 = nn.ReLU()\n",
    "        # self.relu_2 = nn.ReLU()\n",
    "        self.relu_1 = nn.SiLU()\n",
    "        self.relu_2 = nn.SiLU()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=self.planes,\n",
    "            out_channels=self.planes,\n",
    "            kernel_size=fixed_kernel_size,\n",
    "            stride=2,\n",
    "            padding=2,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.block = self._make_resnet_layer(\n",
    "            kernel_size=fixed_kernel_size,\n",
    "            stride=1,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            padding=fixed_kernel_size // 2,\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=self.planes)\n",
    "        self.avgpool = nn.AvgPool1d(kernel_size=6, stride=6, padding=2)\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.in_channels,\n",
    "            hidden_size=128,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            # dropout=0.2,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(in_features=linear_layer_features, out_features=num_classes)\n",
    "\n",
    "    def _make_resnet_layer(\n",
    "        self,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "        blocks=9,\n",
    "        padding=0,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        layers = []\n",
    "        downsample = None\n",
    "        base_width = self.planes\n",
    "\n",
    "        for i in range(blocks):\n",
    "            downsampling = nn.Sequential(\n",
    "                nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "            )\n",
    "            layers.append(\n",
    "                ResNet_1D_Block(\n",
    "                    in_channels=self.planes,\n",
    "                    out_channels=self.planes,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    padding=padding,\n",
    "                    downsampling=downsampling,\n",
    "                    dilation=dilation,\n",
    "                    groups=groups,\n",
    "                    dropout=dropout,\n",
    "                )\n",
    "            )\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        out_sep = []\n",
    "        for i in range(len(self.kernels)):\n",
    "            sep = self.parallel_conv[i](x)\n",
    "            out_sep.append(sep)\n",
    "\n",
    "        out = torch.cat(out_sep, dim=2)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu_1(out)\n",
    "        out = self.conv1(out)\n",
    "\n",
    "        out = self.block(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu_2(out)\n",
    "        out = self.avgpool(out)\n",
    "\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        rnn_out, _ = self.rnn(x.permute(0, 2, 1))\n",
    "        new_rnn_h = rnn_out[:, -1, :]\n",
    "\n",
    "        new_out = torch.cat([out, new_rnn_h], dim=1)\n",
    "        return new_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_out = self.extract_features(x)\n",
    "        result = self.fc(new_out)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07062ec",
   "metadata": {
    "papermill": {
     "duration": 0.01485,
     "end_time": "2024-03-12T09:01:52.852177",
     "exception": false,
     "start_time": "2024-03-12T09:01:52.837327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Adan Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "442fc2d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:01:52.884539Z",
     "iopub.status.busy": "2024-03-12T09:01:52.884232Z",
     "iopub.status.idle": "2024-03-12T09:01:52.909870Z",
     "shell.execute_reply": "2024-03-12T09:01:52.909102Z"
    },
    "papermill": {
     "duration": 0.044477,
     "end_time": "2024-03-12T09:01:52.911794",
     "exception": false,
     "start_time": "2024-03-12T09:01:52.867317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Adan(Optimizer):\n",
    "    \"\"\"\n",
    "    Implements a pytorch variant of Adan\n",
    "    Adan was proposed in\n",
    "    Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models[J]. arXiv preprint arXiv:2208.06677, 2022.\n",
    "    https://arxiv.org/abs/2208.06677\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining parameter groups.\n",
    "        lr (float, optional): learning rate. (default: 1e-3)\n",
    "        betas (Tuple[float, float, flot], optional): coefficients used for computing\n",
    "            running averages of gradient and its norm. (default: (0.98, 0.92, 0.99))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability. (default: 1e-8)\n",
    "        weight_decay (float, optional): decoupled weight decay (L2 penalty) (default: 0)\n",
    "        max_grad_norm (float, optional): value used to clip\n",
    "            global grad norm (default: 0.0 no clip)\n",
    "        no_prox (bool): how to perform the decoupled weight decay (default: False)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=1e-3,\n",
    "        betas=(0.98, 0.92, 0.99),\n",
    "        eps=1e-8,\n",
    "        weight_decay=0.2,\n",
    "        max_grad_norm=0.0,\n",
    "        no_prox=False,\n",
    "    ):\n",
    "        if not 0.0 <= max_grad_norm:\n",
    "            raise ValueError(\"Invalid Max grad norm: {}\".format(max_grad_norm))\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        if not 0.0 <= betas[2] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 2: {}\".format(betas[2]))\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            betas=betas,\n",
    "            eps=eps,\n",
    "            weight_decay=weight_decay,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            no_prox=no_prox,\n",
    "        )\n",
    "        super(Adan, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Adan, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault(\"no_prox\", False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def restart_opt(self):\n",
    "        for group in self.param_groups:\n",
    "            group[\"step\"] = 0\n",
    "            for p in group[\"params\"]:\n",
    "                if p.requires_grad:\n",
    "                    state = self.state[p]\n",
    "                    # State initialization\n",
    "\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p)\n",
    "                    # Exponential moving average of gradient difference\n",
    "                    state[\"exp_avg_diff\"] = torch.zeros_like(p)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        if self.defaults[\"max_grad_norm\"] > 0:\n",
    "            device = self.param_groups[0][\"params\"][0].device\n",
    "            global_grad_norm = torch.zeros(1, device=device)\n",
    "\n",
    "            max_grad_norm = torch.tensor(self.defaults[\"max_grad_norm\"], device=device)\n",
    "            for group in self.param_groups:\n",
    "\n",
    "                for p in group[\"params\"]:\n",
    "                    if p.grad is not None:\n",
    "                        grad = p.grad\n",
    "                        global_grad_norm.add_(grad.pow(2).sum())\n",
    "\n",
    "            global_grad_norm = torch.sqrt(global_grad_norm)\n",
    "\n",
    "            clip_global_grad_norm = torch.clamp(\n",
    "                max_grad_norm / (global_grad_norm + group[\"eps\"]), max=1.0\n",
    "            )\n",
    "        else:\n",
    "            clip_global_grad_norm = 1.0\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            beta1, beta2, beta3 = group[\"betas\"]\n",
    "            # assume same step across group now to simplify things\n",
    "            # per parameter step can be easily support by making it tensor, or pass list into kernel\n",
    "            if \"step\" in group:\n",
    "                group[\"step\"] += 1\n",
    "            else:\n",
    "                group[\"step\"] = 1\n",
    "\n",
    "            bias_correction1 = 1.0 - beta1 ** group[\"step\"]\n",
    "            bias_correction2 = 1.0 - beta2 ** group[\"step\"]\n",
    "            bias_correction3 = 1.0 - beta3 ** group[\"step\"]\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p)\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p)\n",
    "                    state[\"exp_avg_diff\"] = torch.zeros_like(p)\n",
    "\n",
    "                grad = p.grad.mul_(clip_global_grad_norm)\n",
    "                if \"pre_grad\" not in state or group[\"step\"] == 1:\n",
    "                    state[\"pre_grad\"] = grad\n",
    "\n",
    "                copy_grad = grad.clone()\n",
    "\n",
    "                exp_avg, exp_avg_sq, exp_avg_diff = (\n",
    "                    state[\"exp_avg\"],\n",
    "                    state[\"exp_avg_sq\"],\n",
    "                    state[\"exp_avg_diff\"],\n",
    "                )\n",
    "                diff = grad - state[\"pre_grad\"]\n",
    "\n",
    "                update = grad + beta2 * diff\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  # m_t\n",
    "                exp_avg_diff.mul_(beta2).add_(diff, alpha=1 - beta2)  # diff_t\n",
    "                exp_avg_sq.mul_(beta3).addcmul_(update, update, value=1 - beta3)  # n_t\n",
    "\n",
    "                denom = ((exp_avg_sq).sqrt() / math.sqrt(bias_correction3)).add_(\n",
    "                    group[\"eps\"]\n",
    "                )\n",
    "                update = (\n",
    "                    (\n",
    "                        exp_avg / bias_correction1\n",
    "                        + beta2 * exp_avg_diff / bias_correction2\n",
    "                    )\n",
    "                ).div_(denom)\n",
    "\n",
    "                if group[\"no_prox\"]:\n",
    "                    p.data.mul_(1 - group[\"lr\"] * group[\"weight_decay\"])\n",
    "                    p.add_(update, alpha=-group[\"lr\"])\n",
    "                else:\n",
    "                    p.add_(update, alpha=-group[\"lr\"])\n",
    "                    p.data.div_(1 + group[\"lr\"] * group[\"weight_decay\"])\n",
    "\n",
    "                state[\"pre_grad\"] = copy_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bdbbd7",
   "metadata": {
    "papermill": {
     "duration": 0.014178,
     "end_time": "2024-03-12T09:01:52.940528",
     "exception": false,
     "start_time": "2024-03-12T09:01:52.926350",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59afbbdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:01:52.971115Z",
     "iopub.status.busy": "2024-03-12T09:01:52.970344Z",
     "iopub.status.idle": "2024-03-12T09:01:52.981406Z",
     "shell.execute_reply": "2024-03-12T09:01:52.980587Z"
    },
    "papermill": {
     "duration": 0.028261,
     "end_time": "2024-03-12T09:01:52.983214",
     "exception": false,
     "start_time": "2024-03-12T09:01:52.954953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    stage, fold, train_loader, model, criterion, optimizer, epoch, scheduler, device\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        eegs = batch[\"eeg\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            y_preds = model(eegs)\n",
    "            loss = criterion(F.log_softmax(y_preds, dim=1), labels)\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), CFG.max_grad_norm\n",
    "        )\n",
    "\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            global_step += 1\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        end = time.time()\n",
    "\n",
    "        if CFG.log_show and (\n",
    "            step % CFG.log_step == 0 or step == (len(train_loader) - 1)\n",
    "        ):\n",
    "            # remain=timeSince(start, float(step + 1) / len(train_loader))\n",
    "            LOGGER.info(\n",
    "                f\"Epoch {epoch+1} [{step}/{len(train_loader)}] Loss: {losses.val:.4f} Loss Avg:{losses.avg:.4f}\"\n",
    "            )\n",
    "            # \"Elapsed {remain:s} Grad: {grad_norm:.4f}  LR: {cheduler.get_lr()[0]:.8f}\"\n",
    "\n",
    "        if CFG.wandb:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    f\"[fold{fold}] loss\": losses.val,\n",
    "                    f\"[fold{fold}] lr\": scheduler.get_lr()[0],\n",
    "                }\n",
    "            )\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b63e956",
   "metadata": {
    "papermill": {
     "duration": 0.01478,
     "end_time": "2024-03-12T09:01:53.012483",
     "exception": false,
     "start_time": "2024-03-12T09:01:52.997703",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Valid Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab15e0c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:01:53.043778Z",
     "iopub.status.busy": "2024-03-12T09:01:53.043004Z",
     "iopub.status.idle": "2024-03-12T09:01:53.052629Z",
     "shell.execute_reply": "2024-03-12T09:01:53.051706Z"
    },
    "papermill": {
     "duration": 0.027128,
     "end_time": "2024-03-12T09:01:53.054487",
     "exception": false,
     "start_time": "2024-03-12T09:01:53.027359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def valid_fn(stage, epoch, valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    targets = []\n",
    "    start = end = time.time()\n",
    "\n",
    "    for step, batch in enumerate(valid_loader):\n",
    "        eegs = batch[\"eeg\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(eegs)\n",
    "            loss = criterion(F.log_softmax(y_preds, dim=1), labels)\n",
    "\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(nn.Softmax(dim=1)(y_preds).to(\"cpu\").numpy())\n",
    "        targets.append(labels.to(\"cpu\").numpy())\n",
    "        end = time.time()\n",
    "\n",
    "        if CFG.log_show and (\n",
    "            step % CFG.log_step == 0 or step == (len(valid_loader) - 1)\n",
    "        ):\n",
    "            # remain=timeSince(start, float(step + 1) / len(valid_loader))\n",
    "            LOGGER.info(\n",
    "                f\"Epoch {epoch+1} VALIDATION: [{step}/{len(valid_loader)}] Val Loss: {losses.val:.4f} Val Loss Avg: {losses.avg:.4f}\"\n",
    "            )\n",
    "            # Elapsed {remain:s}\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    targets = np.concatenate(targets)\n",
    "\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926656b4",
   "metadata": {
    "papermill": {
     "duration": 0.014319,
     "end_time": "2024-03-12T09:01:53.122454",
     "exception": false,
     "start_time": "2024-03-12T09:01:53.108135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Build Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35b938aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:01:53.153122Z",
     "iopub.status.busy": "2024-03-12T09:01:53.152755Z",
     "iopub.status.idle": "2024-03-12T09:01:53.162325Z",
     "shell.execute_reply": "2024-03-12T09:01:53.161438Z"
    },
    "papermill": {
     "duration": 0.027142,
     "end_time": "2024-03-12T09:01:53.164233",
     "exception": false,
     "start_time": "2024-03-12T09:01:53.137091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_optimizer(cfg, model, device, epochs, num_batches_per_epoch):\n",
    "    lr = cfg.lr\n",
    "    # lr = default_configs[\"lr\"]\n",
    "    if cfg.optimizer == \"SAM\":\n",
    "        base_optimizer = (\n",
    "            torch.optim.SGD\n",
    "        )  # define an optimizer for the \"sharpness-aware\" update\n",
    "        optimizer_model = SAM(\n",
    "            model.parameters(),\n",
    "            base_optimizer,\n",
    "            lr=lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=cfg.weight_decay,\n",
    "            adaptive=True,\n",
    "        )\n",
    "    elif cfg.optimizer == \"Ranger21\":\n",
    "        optimizer_model = Ranger21(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=cfg.weight_decay,\n",
    "            num_epochs=epochs,\n",
    "            num_batches_per_epoch=num_batches_per_epoch,\n",
    "        )\n",
    "    elif cfg.optimizer == \"SGD\":\n",
    "        optimizer_model = torch.optim.SGD(\n",
    "            model.parameters(), lr=lr, weight_decay=cfg.weight_decay, momentum=0.9\n",
    "        )\n",
    "    elif cfg.optimizer == \"Adam\":\n",
    "        optimizer_model = Adam(model.parameters(), lr=lr, weight_decay=CFG.weight_decay)\n",
    "    elif cfg.optimizer == \"AdamW\":\n",
    "        optimizer_model = AdamW(\n",
    "            model.parameters(), lr=lr, weight_decay=CFG.weight_decay\n",
    "        )\n",
    "    elif cfg.optimizer == \"Lion\":\n",
    "        optimizer_model = Lion(model.parameters(), lr=lr, weight_decay=cfg.weight_decay)\n",
    "    elif cfg.optimizer == \"Adan\":\n",
    "        optimizer_model = Adan(model.parameters(), lr=lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    return optimizer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b4df7",
   "metadata": {
    "papermill": {
     "duration": 0.014393,
     "end_time": "2024-03-12T09:01:53.193447",
     "exception": false,
     "start_time": "2024-03-12T09:01:53.179054",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f67ed56b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:01:53.224325Z",
     "iopub.status.busy": "2024-03-12T09:01:53.223572Z",
     "iopub.status.idle": "2024-03-12T09:01:53.229879Z",
     "shell.execute_reply": "2024-03-12T09:01:53.229052Z"
    },
    "papermill": {
     "duration": 0.024114,
     "end_time": "2024-03-12T09:01:53.231902",
     "exception": false,
     "start_time": "2024-03-12T09:01:53.207788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_scheduler(optimizer, epochs, steps_per_epoch):\n",
    "    if CFG.scheduler == \"ReduceLROnPlateau\":\n",
    "        scheduler = ReduceLROnPlateau(optimizer, **CFG.reduce_params)\n",
    "    elif CFG.scheduler == \"CosineAnnealingLR\":\n",
    "        scheduler = CosineAnnealingLR(optimizer, **CFG.cosanneal_params)\n",
    "    elif CFG.scheduler == \"CosineAnnealingWarmRestarts\":\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, **CFG.cosanneal_res_params)\n",
    "    elif CFG.scheduler == \"OneCycleLR\":\n",
    "        scheduler = OneCycleLR(\n",
    "            optimizer=optimizer,\n",
    "            epochs=epochs,\n",
    "            pct_start=0.0,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            max_lr=CFG.lr,\n",
    "            div_factor=25,\n",
    "            final_div_factor=4.0e-01,\n",
    "        )\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6d332a",
   "metadata": {
    "papermill": {
     "duration": 0.014438,
     "end_time": "2024-03-12T09:01:53.260746",
     "exception": false,
     "start_time": "2024-03-12T09:01:53.246308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e0b9d2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:01:53.291249Z",
     "iopub.status.busy": "2024-03-12T09:01:53.290988Z",
     "iopub.status.idle": "2024-03-12T09:01:53.311808Z",
     "shell.execute_reply": "2024-03-12T09:01:53.311050Z"
    },
    "papermill": {
     "duration": 0.038188,
     "end_time": "2024-03-12T09:01:53.313680",
     "exception": false,
     "start_time": "2024-03-12T09:01:53.275492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop(stage, epochs, folds, fold, directory, prev_dir, eggs):\n",
    "    train_folds = folds[folds[\"fold\"] != fold].reset_index(drop=True)\n",
    "    valid_folds = folds[folds[\"fold\"] == fold].reset_index(drop=True)\n",
    "    valid_labels = valid_folds[CFG.target_cols].values\n",
    "\n",
    "    train_dataset = EEGDataset(\n",
    "        train_folds,\n",
    "        batch_size=CFG.batch_size,\n",
    "        mode=\"train\",\n",
    "        eegs=eggs,\n",
    "        bandpass_filter=CFG.bandpass_filter,\n",
    "        rand_filter=CFG.rand_filter,\n",
    "    )\n",
    "        \n",
    "    valid_dataset = EEGDataset(\n",
    "        valid_folds,\n",
    "        batch_size=CFG.batch_size,\n",
    "        mode=\"valid\",\n",
    "        eegs=eggs,\n",
    "        bandpass_filter=CFG.bandpass_filter,\n",
    "        #rand_filter=CFG.rand_filter,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=CFG.batch_size * CFG.batch_koef_valid,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    LOGGER.info(\n",
    "        f\"========== stage: {stage} fold: {fold} training {len(train_loader)} / {len(valid_loader)} ==========\"\n",
    "    )\n",
    "\n",
    "    model = EEGNet(\n",
    "        kernels=CFG.kernels,\n",
    "        in_channels=CFG.in_channels,\n",
    "        fixed_kernel_size=CFG.fixed_kernel_size,\n",
    "        num_classes=CFG.target_size,\n",
    "        linear_layer_features=CFG.linear_layer_features,\n",
    "    )\n",
    "\n",
    "    if stage > 1:\n",
    "        model_weight = f\"{prev_dir}{CFG.model_name}_ver-{CFG.VERSION}_stage-{stage-1}_fold-{fold}_best.pth\"\n",
    "        checkpoint = torch.load(model_weight, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # CPMP: wrap the model to use all GPUs\n",
    "    if CFG.parallel:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    optimizer = build_optimizer(\n",
    "        CFG, model, device, epochs=epochs, num_batches_per_epoch=len(train_loader)\n",
    "    )\n",
    "    scheduler = get_scheduler(\n",
    "        optimizer, epochs=epochs, steps_per_epoch=len(train_loader)\n",
    "    )\n",
    "    criterion = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "    best_score = np.inf\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(\n",
    "            stage,\n",
    "            fold,\n",
    "            train_loader,\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            scheduler,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        # eval\n",
    "        valid_dataset.set_offset(CFG.sample_offset)\n",
    "        avg_val_loss, predictions = valid_fn(\n",
    "            stage,\n",
    "            epoch,\n",
    "            valid_loader,\n",
    "            model,\n",
    "            criterion,\n",
    "            device,\n",
    "        )\n",
    "        \n",
    "        avg_loss_line = ''\n",
    "        if CFG.multi_validation:\n",
    "            multi_avg_val_loss = np.zeros(CFG.n_split_samples)\n",
    "            start = (2 * CFG.sample_delta) // CFG.n_split_samples\n",
    "            finish = (3 * CFG.sample_delta) // CFG.n_split_samples\n",
    "            delta = (finish - start) // 5\n",
    "            for i in range(CFG.n_split_samples):\n",
    "                valid_dataset.set_offset(start)\n",
    "                multi_avg_val_loss[i], _ = valid_fn(\n",
    "                    stage,\n",
    "                    epoch,\n",
    "                    valid_loader,\n",
    "                    model,\n",
    "                    criterion,\n",
    "                    device,\n",
    "                )\n",
    "                avg_loss_line += f\" {multi_avg_val_loss[i]:.4f}\"\n",
    "                start += delta\n",
    "            avg_loss_line += f\" mean={np.mean(multi_avg_val_loss):.4f}\"\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(\n",
    "            f\"Epoch {epoch+1} Avg Train Loss: {avg_loss:.4f} Avg Valid Loss: {avg_val_loss:.4f} / {avg_loss_line}\"\n",
    "        )\n",
    "        #   time: {elapsed:.0f}s\n",
    "        if CFG.wandb:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    f\"[fold{fold}] stage\": stage,\n",
    "                    f\"[fold{fold}] epoch\": epoch + 1,\n",
    "                    f\"[fold{fold}] avg_train_loss\": avg_loss,\n",
    "                    f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
    "                    #f\"[fold{fold}] score\": score,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if CFG.save_all_models:\n",
    "            torch.save(\n",
    "                {\"model\": model.module.state_dict(), \"predictions\": predictions},\n",
    "                f\"{directory}{CFG.model_name}_ver-{CFG.VERSION}_stage-{stage}_fold-{fold}_epoch-{epoch}_val-{avg_val_loss:.4f}_train-{avg_loss:.4f}.pth\",\n",
    "            )\n",
    "\n",
    "        if best_score > avg_val_loss:\n",
    "            best_score = avg_val_loss\n",
    "            LOGGER.info(f\"Epoch {epoch+1} Save Best Valid Loss: {avg_val_loss:.4f}\")\n",
    "            # CPMP: save the original model. It is stored as the module attribute of the DP model.\n",
    "            torch.save(\n",
    "                {\"model\": model.module.state_dict(), \"predictions\": predictions},\n",
    "                f\"{directory}{CFG.model_name}_ver-{CFG.VERSION}_stage-{stage}_fold-{fold}_best.pth\",\n",
    "            )\n",
    "\n",
    "    predictions = torch.load(\n",
    "        f\"{directory}{CFG.model_name}_ver-{CFG.VERSION}_stage-{stage}_fold-{fold}_best.pth\",\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )[\"predictions\"]\n",
    "\n",
    "    # valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n",
    "    valid_folds[CFG.pred_cols] = predictions\n",
    "    valid_folds[CFG.target_cols] = valid_labels\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return valid_folds, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8638de",
   "metadata": {
    "papermill": {
     "duration": 0.014333,
     "end_time": "2024-03-12T09:01:53.342673",
     "exception": false,
     "start_time": "2024-03-12T09:01:53.328340",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d44a688",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:01:53.373717Z",
     "iopub.status.busy": "2024-03-12T09:01:53.373176Z",
     "iopub.status.idle": "2024-03-12T09:01:53.831464Z",
     "shell.execute_reply": "2024-03-12T09:01:53.830365Z"
    },
    "papermill": {
     "duration": 0.475989,
     "end_time": "2024-03-12T09:01:53.833512",
     "exception": false,
     "start_time": "2024-03-12T09:01:53.357523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (106800, 15)\n",
      "Targets ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n",
      "There are 1950 patients in the training data.\n",
      "There are 17089 EEG IDs in the training data.\n",
      "There are 20183 unique eeg_id + votes in the training data.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(CFG.file_train)\n",
    "TARGETS = train.columns[-6:]\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Targets\", list(TARGETS))\n",
    "\n",
    "train[\"total_evaluators\"] = train[CFG.target_cols].sum(axis=1)\n",
    "\n",
    "train_uniq = train.drop_duplicates(subset=[\"eeg_id\"] + list(TARGETS))\n",
    "\n",
    "print(f\"There are {train.patient_id.nunique()} patients in the training data.\")\n",
    "print(f\"There are {train.eeg_id.nunique()} EEG IDs in the training data.\")\n",
    "print(f\"There are {train_uniq.shape[0]} unique eeg_id + votes in the training data.\")\n",
    "\n",
    "if CFG.visualize:\n",
    "    train_uniq.eeg_id.value_counts().value_counts().plot(\n",
    "        kind=\"bar\",\n",
    "        title=f\"Distribution of Count of EEG w Unique Vote: \"\n",
    "        f\"{train_uniq.shape[0]} examples\",\n",
    "    )\n",
    "\n",
    "# del train_uniq\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74e3bae4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:01:53.864871Z",
     "iopub.status.busy": "2024-03-12T09:01:53.864267Z",
     "iopub.status.idle": "2024-03-12T09:01:54.257179Z",
     "shell.execute_reply": "2024-03-12T09:01:54.256173Z"
    },
    "papermill": {
     "duration": 0.410919,
     "end_time": "2024-03-12T09:01:54.259450",
     "exception": false,
     "start_time": "2024-03-12T09:01:53.848531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20 raw eeg features\n",
      "['Fp1', 'F3', 'C3', 'P3', 'F7', 'T3', 'T5', 'O1', 'Fz', 'Cz', 'Pz', 'Fp2', 'F4', 'C4', 'P4', 'F8', 'T4', 'T6', 'O2', 'EKG']\n"
     ]
    }
   ],
   "source": [
    "if CFG.visualize:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(train[\"total_evaluators\"], bins=10, color=\"blue\", edgecolor=\"black\")\n",
    "    plt.title(\"Histogram of Total Evaluators\")\n",
    "    plt.xlabel(\"Total Evaluators\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "tst_eeg_df = pd.read_parquet(CFG.file_features_test)\n",
    "tst_eeg_features = tst_eeg_df.columns\n",
    "print(f\"There are {len(tst_eeg_features)} raw eeg features\")\n",
    "print(list(tst_eeg_features))\n",
    "del tst_eeg_df\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dd78e9",
   "metadata": {
    "papermill": {
     "duration": 0.014707,
     "end_time": "2024-03-12T09:01:54.289246",
     "exception": false,
     "start_time": "2024-03-12T09:01:54.274539",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03bc746b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:01:54.320017Z",
     "iopub.status.busy": "2024-03-12T09:01:54.319723Z",
     "iopub.status.idle": "2024-03-12T09:01:54.401307Z",
     "shell.execute_reply": "2024-03-12T09:01:54.400454Z"
    },
    "papermill": {
     "duration": 0.099335,
     "end_time": "2024-03-12T09:01:54.403280",
     "exception": false,
     "start_time": "2024-03-12T09:01:54.303945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>eeg_sub_id</th>\n",
       "      <th>eeg_label_offset_seconds</th>\n",
       "      <th>spectrogram_id</th>\n",
       "      <th>spectrogram_sub_id</th>\n",
       "      <th>spectrogram_label_offset_seconds</th>\n",
       "      <th>label_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>seizure_vote</th>\n",
       "      <th>lpd_vote</th>\n",
       "      <th>gpd_vote</th>\n",
       "      <th>lrda_vote</th>\n",
       "      <th>grda_vote</th>\n",
       "      <th>other_vote</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expert_consensus</th>\n",
       "      <th>total_evaluators</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [eeg_id, eeg_sub_id, eeg_label_offset_seconds, spectrogram_id, spectrogram_sub_id, spectrogram_label_offset_seconds, label_id, patient_id, seizure_vote, lpd_vote, gpd_vote, lrda_vote, grda_vote, other_vote, target]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "# all_eeg_specs = np.load(CFG.file_eeg_specs, allow_pickle=True).item()\n",
    "\n",
    "# train = train[train[\"label_id\"].isin(all_eeg_specs.keys())].copy()\n",
    "# print(train.shape[0])\n",
    "\n",
    "train = train_uniq\n",
    "\n",
    "# Load and apply pseudo labels\n",
    "mask = train['total_evaluators'] < 10\n",
    "\n",
    "pseudo_labels = np.load('/kaggle/input/mixmodel-weights/pseudo_labels.npy')\n",
    "pseudo_labels = F.softmax(torch.tensor(pseudo_labels), dim=1).numpy()\n",
    "\n",
    "# y_data = train[TARGETS].values + 0.166666667  # Regularization value\n",
    "y_data = train[TARGETS].values + np.multiply(np.expand_dims(mask, axis=1), pseudo_labels)\n",
    "y_data = y_data / y_data.sum(axis=1, keepdims=True)\n",
    "train[TARGETS] = y_data\n",
    "\n",
    "train[\"target\"] = train[\"expert_consensus\"]\n",
    "\n",
    "train[train['total_evaluators'] == CFG.test_total_eval].groupby(['expert_consensus','total_evaluators']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46b7f45e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:01:54.435675Z",
     "iopub.status.busy": "2024-03-12T09:01:54.434977Z",
     "iopub.status.idle": "2024-03-12T09:01:54.443263Z",
     "shell.execute_reply": "2024-03-12T09:01:54.442547Z"
    },
    "papermill": {
     "duration": 0.026823,
     "end_time": "2024-03-12T09:01:54.445102",
     "exception": false,
     "start_time": "2024-03-12T09:01:54.418279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.test_total_eval > 0:\n",
    "    train['key_id'] = range(train.shape[0])\n",
    "\n",
    "    train_pop_olds = []\n",
    "    for total_eval in CFG.total_evals_old:\n",
    "        if type(total_eval) is list:\n",
    "            pop_idx = (train[\"total_evaluators\"] >= total_eval[0][0]) & (\n",
    "                train[\"total_evaluators\"] < total_eval[0][1]\n",
    "            ) | (train[\"total_evaluators\"] >= total_eval[1][0]) & (\n",
    "                train[\"total_evaluators\"] < total_eval[1][1]\n",
    "            )\n",
    "        else:\n",
    "            pop_idx = (train[\"total_evaluators\"] >= total_eval[0]) & (\n",
    "                train[\"total_evaluators\"] < total_eval[1]\n",
    "            )\n",
    "\n",
    "        train_pop = train[pop_idx].copy().reset_index()\n",
    "\n",
    "        sgkf = GroupKFold(n_splits=CFG.n_fold)\n",
    "        train_pop[\"fold\"] = -1\n",
    "        for fold_id, (_, val_idx) in enumerate(\n",
    "            sgkf.split(train_pop, y=train_pop[\"target\"], groups=train_pop[\"patient_id\"])\n",
    "        ):\n",
    "            train_pop.loc[val_idx, \"fold\"] = fold_id\n",
    "\n",
    "        train_pop_olds.append(train_pop)\n",
    "        print(train_pop.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "897ebf6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:01:54.475630Z",
     "iopub.status.busy": "2024-03-12T09:01:54.475360Z",
     "iopub.status.idle": "2024-03-12T09:01:54.530036Z",
     "shell.execute_reply": "2024-03-12T09:01:54.529178Z"
    },
    "papermill": {
     "duration": 0.07224,
     "end_time": "2024-03-12T09:01:54.532112",
     "exception": false,
     "start_time": "2024-03-12T09:01:54.459872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20183\n",
      "6350\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>eeg_sub_id</th>\n",
       "      <th>eeg_label_offset_seconds</th>\n",
       "      <th>spectrogram_id</th>\n",
       "      <th>spectrogram_sub_id</th>\n",
       "      <th>spectrogram_label_offset_seconds</th>\n",
       "      <th>label_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>seizure_vote</th>\n",
       "      <th>lpd_vote</th>\n",
       "      <th>gpd_vote</th>\n",
       "      <th>lrda_vote</th>\n",
       "      <th>grda_vote</th>\n",
       "      <th>other_vote</th>\n",
       "      <th>target</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expert_consensus</th>\n",
       "      <th>total_evaluators</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index, eeg_id, eeg_sub_id, eeg_label_offset_seconds, spectrogram_id, spectrogram_sub_id, spectrogram_label_offset_seconds, label_id, patient_id, seizure_vote, lpd_vote, gpd_vote, lrda_vote, grda_vote, other_vote, target, fold]\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pops = []\n",
    "for eval_list in CFG.total_evaluators:\n",
    "    result=[]\n",
    "    train_pop = train  \n",
    "    for eval_dict in eval_list:\n",
    "        band = eval_dict['band']\n",
    "        pop_idx = (train_pop[\"total_evaluators\"] >= band[0]) \n",
    "        pop_idx &= (train_pop[\"total_evaluators\"] <= band[1])\n",
    "        for exclude in eval_dict['excl_evals']:\n",
    "            pop_idx &= ~(train_pop['expert_consensus'] == exclude)\n",
    "            pass\n",
    "        result.append(train_pop[pop_idx])\n",
    "    train_pop = pd.concat(result).copy().reset_index()\n",
    "\n",
    "    sgkf = GroupKFold(n_splits=CFG.n_fold)\n",
    "    train_pop[\"fold\"] = -1\n",
    "    for fold_id, (_, val_idx) in enumerate(\n",
    "        sgkf.split(train_pop, y=train_pop[\"target\"], groups=train_pop[\"patient_id\"])\n",
    "    ):\n",
    "        train_pop.loc[val_idx, \"fold\"] = fold_id\n",
    "\n",
    "    train_pops.append(train_pop)\n",
    "    print(train_pop.shape[0])\n",
    "\n",
    "train_0 = train_pops[0]\n",
    "train_0[train_0['total_evaluators'] == CFG.test_total_eval].groupby(['expert_consensus','total_evaluators']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7867d90c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:01:54.565144Z",
     "iopub.status.busy": "2024-03-12T09:01:54.564470Z",
     "iopub.status.idle": "2024-03-12T09:01:54.570605Z",
     "shell.execute_reply": "2024-03-12T09:01:54.569793Z"
    },
    "papermill": {
     "duration": 0.024323,
     "end_time": "2024-03-12T09:01:54.572498",
     "exception": false,
     "start_time": "2024-03-12T09:01:54.548175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.test_total_eval > 0:\n",
    "    df_old = train_pop_olds[0].copy(deep=True).set_index(['key_id'], drop=True).drop(columns=['fold'])\n",
    "    df_new = train_pops[0].copy(deep=True).set_index(['key_id'], drop=True).drop(columns=['fold'])\n",
    "\n",
    "    #outer merge the two DataFrames, adding an indicator column called 'Exist'\n",
    "    diff_df = pd.merge(df_old, df_new, how='outer', indicator='Exist')\n",
    "\n",
    "    #find which rows don't exist in both DataFrames\n",
    "    diff_df = diff_df.loc[diff_df['Exist'] != 'both']\n",
    "    display(diff_df)\n",
    "\n",
    "    del df_old, df_new, diff_df, train_pop_olds\n",
    "    _ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ddcc5de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:01:54.604301Z",
     "iopub.status.busy": "2024-03-12T09:01:54.604029Z",
     "iopub.status.idle": "2024-03-12T09:01:54.741840Z",
     "shell.execute_reply": "2024-03-12T09:01:54.740877Z"
    },
    "papermill": {
     "duration": 0.156118,
     "end_time": "2024-03-12T09:01:54.744047",
     "exception": false,
     "start_time": "2024-03-12T09:01:54.587929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.visualize:\n",
    "    print(\"Pop 1: train unique eeg_id + votes shape:\", train_pops[0].shape)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(train[\"total_evaluators\"], bins=10, color=\"blue\", edgecolor=\"black\")\n",
    "    plt.title(\"Histogram of Total Evaluators\")\n",
    "    plt.xlabel(\"Total Evaluators\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# del all_eeg_specs\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d7bf7c",
   "metadata": {
    "papermill": {
     "duration": 0.015579,
     "end_time": "2024-03-12T09:01:54.775950",
     "exception": false,
     "start_time": "2024-03-12T09:01:54.760371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Deduplicate Train EEG Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62da45d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:01:54.808417Z",
     "iopub.status.busy": "2024-03-12T09:01:54.807669Z",
     "iopub.status.idle": "2024-03-12T09:03:35.078697Z",
     "shell.execute_reply": "2024-03-12T09:03:35.077720Z"
    },
    "papermill": {
     "duration": 100.289745,
     "end_time": "2024-03-12T09:03:35.081136",
     "exception": false,
     "start_time": "2024-03-12T09:01:54.791391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "if CFG.create_eegs:\n",
    "    all_eegs = {}\n",
    "    visualize = 1 if CFG.visualize else 0\n",
    "    eeg_ids = train.eeg_id.unique()\n",
    "\n",
    "    for i, eeg_id in tqdm(enumerate(eeg_ids)):\n",
    "\n",
    "        # Сохранить ЭЭГ в словаре Python для массивов numpy\n",
    "        eeg_path = CFG.path_train / f\"{eeg_id}.parquet\"\n",
    "\n",
    "        # Вырезаем среднюю 50 секундную часть и заполняем по среднему Nan\n",
    "        data = eeg_from_parquet(eeg_path, display=i < visualize)\n",
    "        all_eegs[eeg_id] = data\n",
    "\n",
    "        if i == visualize:\n",
    "            if CFG.create_eegs:\n",
    "                print(\n",
    "                    f\"Processing {train['eeg_id'].nunique()} eeg parquets... \", end=\"\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Reading {len(eeg_ids)} eeg NumPys from disk.\")\n",
    "                break\n",
    "    np.save(\"./eegs\", all_eegs)\n",
    "\n",
    "else:\n",
    "    all_eegs = np.load(CFG.file_raw_eeg, allow_pickle=True).item()\n",
    "\n",
    "if CFG.visualize:\n",
    "    frequencies = [1, 2, 4, 8, 16][::-1]  # frequencies in Hz\n",
    "    x = [all_eegs[eeg_ids[0]][:, 0]]  # select one EEG feature\n",
    "\n",
    "    for frequency in frequencies:\n",
    "        x.append(butter_lowpass_filter(x[0], cutoff_freq=frequency))\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(range(CFG.nsamples), x[0], label=\"without filter\")\n",
    "    for k in range(1, len(x)):\n",
    "        plt.plot(\n",
    "            range(CFG.nsamples),\n",
    "            x[k] - k * (x[0].max() - x[0].min()),\n",
    "            label=f\"with filter {frequencies[k-1]}Hz\",\n",
    "        )\n",
    "\n",
    "    plt.legend()\n",
    "    plt.yticks([])\n",
    "    plt.title(\"Butter Low-Pass Filter Examples\", size=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fbd9754",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:03:35.114094Z",
     "iopub.status.busy": "2024-03-12T09:03:35.113813Z",
     "iopub.status.idle": "2024-03-12T09:03:35.124878Z",
     "shell.execute_reply": "2024-03-12T09:03:35.124030Z"
    },
    "papermill": {
     "duration": 0.029396,
     "end_time": "2024-03-12T09:03:35.126880",
     "exception": false,
     "start_time": "2024-03-12T09:03:35.097484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.visualize:\n",
    "    train_dataset = EEGDataset(\n",
    "        train_pops[0], batch_size=CFG.batch_size, eegs=all_eegs, mode=\"train\"\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    output = train_dataset[0]\n",
    "    X, y = output[\"eeg\"], output[\"labels\"]\n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "    iot = torch.randn(2, CFG.nsamples, CFG.in_channels)  # .cuda()\n",
    "    model = EEGNet(\n",
    "        kernels=CFG.kernels,\n",
    "        in_channels=CFG.in_channels,\n",
    "        fixed_kernel_size=CFG.fixed_kernel_size,\n",
    "        num_classes=CFG.target_size,\n",
    "        linear_layer_features=CFG.linear_layer_features,\n",
    "    )\n",
    "    output = model(iot)\n",
    "    print(output.shape)\n",
    "\n",
    "    for batch in train_loader:\n",
    "        X = batch.pop(\"eeg\")\n",
    "        y = batch.pop(\"labels\")\n",
    "        for item in range(4):\n",
    "            plt.figure(figsize=(20, 4))\n",
    "            offset = 0\n",
    "            for col in range(X.shape[-1]):\n",
    "                if col != 0:\n",
    "                    offset -= X[item, :, col].min()\n",
    "                plt.plot(\n",
    "                    range(CFG.nsamples),\n",
    "                    X[item, :, col] + offset,\n",
    "                    label=f\"feature {col+1}\",\n",
    "                )\n",
    "                offset += X[item, :, col].max()\n",
    "            tt = f\"{y[col][0]:0.1f}\"\n",
    "            for t in y[col][1:]:\n",
    "                tt += f\", {t:0.1f}\"\n",
    "            plt.title(f\"EEG_Id = {eeg_ids[item]}\\nTarget = {tt}\", size=14)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        break\n",
    "\n",
    "    del iot, model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3713964c",
   "metadata": {
    "papermill": {
     "duration": 0.015101,
     "end_time": "2024-03-12T09:03:35.157589",
     "exception": false,
     "start_time": "2024-03-12T09:03:35.142488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe55e95e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:03:35.188946Z",
     "iopub.status.busy": "2024-03-12T09:03:35.188700Z",
     "iopub.status.idle": "2024-03-12T09:03:35.196012Z",
     "shell.execute_reply": "2024-03-12T09:03:35.195187Z"
    },
    "papermill": {
     "duration": 0.025069,
     "end_time": "2024-03-12T09:03:35.197909",
     "exception": false,
     "start_time": "2024-03-12T09:03:35.172840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_score(preds, targets):\n",
    "    oof = pd.DataFrame(preds.copy())\n",
    "    oof[\"id\"] = np.arange(len(oof))\n",
    "    true = pd.DataFrame(targets.copy())\n",
    "    true[\"id\"] = np.arange(len(true))\n",
    "    cv = kaggle_kl_div.score(solution=true, submission=oof, row_id_column_name=\"id\")\n",
    "    return cv\n",
    "\n",
    "\n",
    "def get_result(result_df):\n",
    "    gt = result_df[[\"eeg_id\"] + CFG.target_cols]\n",
    "    gt.sort_values(by=\"eeg_id\", inplace=True)\n",
    "    gt.reset_index(inplace=True, drop=True)\n",
    "    preds = result_df[[\"eeg_id\"] + CFG.pred_cols]\n",
    "    preds.columns = [\"eeg_id\"] + CFG.target_cols\n",
    "    preds.sort_values(by=\"eeg_id\", inplace=True)\n",
    "    preds.reset_index(inplace=True, drop=True)\n",
    "    score_loss = get_score(gt[CFG.target_cols], preds[CFG.target_cols])\n",
    "    LOGGER.info(f\"Score with best loss weights: {score_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8769ec4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:03:35.229934Z",
     "iopub.status.busy": "2024-03-12T09:03:35.229623Z",
     "iopub.status.idle": "2024-03-12T09:03:35.239202Z",
     "shell.execute_reply": "2024-03-12T09:03:35.238370Z"
    },
    "papermill": {
     "duration": 0.027717,
     "end_time": "2024-03-12T09:03:35.241223",
     "exception": false,
     "start_time": "2024-03-12T09:03:35.213506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\" and CFG.train_by_stages:\n",
    "    seed_torch(seed=CFG.seed)\n",
    "\n",
    "    prev_dir = \"\"\n",
    "    for stage in range(len(CFG.total_evaluators)):\n",
    "        pop_dir = f\"{OUTPUT_DIR}pop_{stage+1}_weight_oof/\"\n",
    "        if not os.path.exists(pop_dir):\n",
    "            os.makedirs(pop_dir)\n",
    "\n",
    "        if stage not in CFG.train_stages:\n",
    "            prev_dir = pop_dir\n",
    "            continue\n",
    "\n",
    "        oof_df = pd.DataFrame()\n",
    "        scores = []\n",
    "        for fold in CFG.train_folds:\n",
    "            train_oof_df, score = train_loop(\n",
    "                stage=stage + 1,\n",
    "                epochs=CFG.epochs[stage],\n",
    "                fold=fold,\n",
    "                folds=train_pops[stage],\n",
    "                directory=pop_dir,\n",
    "                prev_dir=prev_dir,\n",
    "                eggs=all_eegs,\n",
    "            )\n",
    "\n",
    "            oof_df = pd.concat([oof_df, train_oof_df])\n",
    "            scores.append(score)\n",
    "\n",
    "            LOGGER.info(f\"========== stage: {stage+1} fold: {fold} result ==========\")\n",
    "            LOGGER.info(f\"Score with best loss weights stage{stage+1}: {score:.4f}\")\n",
    "\n",
    "        LOGGER.info(f\"==================== CV ====================\")\n",
    "        LOGGER.info(f\"Score with best loss weights: {np.mean(scores):.4f}\")\n",
    "\n",
    "        oof_df.reset_index(drop=True, inplace=True)\n",
    "        oof_df.to_csv(\n",
    "            f\"{pop_dir}{CFG.model_name}_oof_df_ver-{CFG.VERSION}_stage-{stage+1}.csv\",\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "        prev_dir = pop_dir\n",
    "\n",
    "    if CFG.wandb:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62fb35a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:03:35.272988Z",
     "iopub.status.busy": "2024-03-12T09:03:35.272740Z",
     "iopub.status.idle": "2024-03-12T13:50:40.658813Z",
     "shell.execute_reply": "2024-03-12T13:50:40.657483Z"
    },
    "papermill": {
     "duration": 17225.404703,
     "end_time": "2024-03-12T13:50:40.661324",
     "exception": false,
     "start_time": "2024-03-12T09:03:35.256621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== stage: 1 fold: 0 training 252 / 32 ==========\n",
      "Epoch 1 Avg Train Loss: 0.6213 Avg Valid Loss: 0.6498 / \n",
      "Epoch 1 Save Best Valid Loss: 0.6498\n",
      "Epoch 2 Avg Train Loss: 0.5083 Avg Valid Loss: 0.6113 / \n",
      "Epoch 2 Save Best Valid Loss: 0.6113\n",
      "Epoch 3 Avg Train Loss: 0.4848 Avg Valid Loss: 0.5369 / \n",
      "Epoch 3 Save Best Valid Loss: 0.5369\n",
      "Epoch 4 Avg Train Loss: 0.4642 Avg Valid Loss: 0.5212 / \n",
      "Epoch 4 Save Best Valid Loss: 0.5212\n",
      "Epoch 5 Avg Train Loss: 0.4544 Avg Valid Loss: 0.5009 / \n",
      "Epoch 5 Save Best Valid Loss: 0.5009\n",
      "Epoch 6 Avg Train Loss: 0.4479 Avg Valid Loss: 0.5261 / \n",
      "Epoch 7 Avg Train Loss: 0.4455 Avg Valid Loss: 0.4909 / \n",
      "Epoch 7 Save Best Valid Loss: 0.4909\n",
      "Epoch 8 Avg Train Loss: 0.4361 Avg Valid Loss: 0.4770 / \n",
      "Epoch 8 Save Best Valid Loss: 0.4770\n",
      "Epoch 9 Avg Train Loss: 0.4310 Avg Valid Loss: 0.4992 / \n",
      "Epoch 10 Avg Train Loss: 0.4304 Avg Valid Loss: 0.4942 / \n",
      "Epoch 11 Avg Train Loss: 0.4226 Avg Valid Loss: 0.4946 / \n",
      "Epoch 12 Avg Train Loss: 0.4219 Avg Valid Loss: 0.5056 / \n",
      "Epoch 13 Avg Train Loss: 0.4181 Avg Valid Loss: 0.5166 / \n",
      "Epoch 14 Avg Train Loss: 0.4115 Avg Valid Loss: 0.5325 / \n",
      "Epoch 15 Avg Train Loss: 0.4149 Avg Valid Loss: 0.4784 / \n",
      "Epoch 16 Avg Train Loss: 0.4119 Avg Valid Loss: 0.5001 / \n",
      "Epoch 17 Avg Train Loss: 0.4098 Avg Valid Loss: 0.5029 / \n",
      "Epoch 18 Avg Train Loss: 0.4062 Avg Valid Loss: 0.4934 / \n",
      "Epoch 19 Avg Train Loss: 0.4070 Avg Valid Loss: 0.5034 / \n",
      "Epoch 20 Avg Train Loss: 0.4071 Avg Valid Loss: 0.4791 / \n",
      "Epoch 21 Avg Train Loss: 0.4089 Avg Valid Loss: 0.4970 / \n",
      "Epoch 22 Avg Train Loss: 0.4024 Avg Valid Loss: 0.4742 / \n",
      "Epoch 22 Save Best Valid Loss: 0.4742\n",
      "Epoch 23 Avg Train Loss: 0.4012 Avg Valid Loss: 0.4671 / \n",
      "Epoch 23 Save Best Valid Loss: 0.4671\n",
      "Epoch 24 Avg Train Loss: 0.3991 Avg Valid Loss: 0.4628 / \n",
      "Epoch 24 Save Best Valid Loss: 0.4628\n",
      "Epoch 25 Avg Train Loss: 0.3968 Avg Valid Loss: 0.4810 / \n",
      "Epoch 26 Avg Train Loss: 0.3938 Avg Valid Loss: 0.4878 / \n",
      "Epoch 27 Avg Train Loss: 0.3973 Avg Valid Loss: 0.4791 / \n",
      "Epoch 28 Avg Train Loss: 0.3983 Avg Valid Loss: 0.4736 / \n",
      "Epoch 29 Avg Train Loss: 0.3910 Avg Valid Loss: 0.4446 / \n",
      "Epoch 29 Save Best Valid Loss: 0.4446\n",
      "Epoch 30 Avg Train Loss: 0.3909 Avg Valid Loss: 0.4384 / \n",
      "Epoch 30 Save Best Valid Loss: 0.4384\n",
      "Epoch 31 Avg Train Loss: 0.3917 Avg Valid Loss: 0.4915 / \n",
      "Epoch 32 Avg Train Loss: 0.3910 Avg Valid Loss: 0.4655 / \n",
      "Epoch 33 Avg Train Loss: 0.3900 Avg Valid Loss: 0.4851 / \n",
      "Epoch 34 Avg Train Loss: 0.3879 Avg Valid Loss: 0.4896 / \n",
      "Epoch 35 Avg Train Loss: 0.3900 Avg Valid Loss: 0.4540 / \n",
      "Epoch 36 Avg Train Loss: 0.3880 Avg Valid Loss: 0.4655 / \n",
      "Epoch 37 Avg Train Loss: 0.3875 Avg Valid Loss: 0.4644 / \n",
      "Epoch 38 Avg Train Loss: 0.3905 Avg Valid Loss: 0.4880 / \n",
      "Epoch 39 Avg Train Loss: 0.3859 Avg Valid Loss: 0.4675 / \n",
      "Epoch 40 Avg Train Loss: 0.3914 Avg Valid Loss: 0.4622 / \n",
      "Epoch 41 Avg Train Loss: 0.3944 Avg Valid Loss: 0.4568 / \n",
      "Epoch 42 Avg Train Loss: 0.3956 Avg Valid Loss: 0.4602 / \n",
      "Epoch 43 Avg Train Loss: 0.3935 Avg Valid Loss: 0.4571 / \n",
      "Epoch 44 Avg Train Loss: 0.3974 Avg Valid Loss: 0.4602 / \n",
      "Epoch 45 Avg Train Loss: 0.3945 Avg Valid Loss: 0.4577 / \n",
      "Epoch 46 Avg Train Loss: 0.3936 Avg Valid Loss: 0.4630 / \n",
      "Epoch 47 Avg Train Loss: 0.3963 Avg Valid Loss: 0.4635 / \n",
      "Epoch 48 Avg Train Loss: 0.3968 Avg Valid Loss: 0.4632 / \n",
      "Epoch 49 Avg Train Loss: 0.3969 Avg Valid Loss: 0.4618 / \n",
      "Epoch 50 Avg Train Loss: 0.3959 Avg Valid Loss: 0.4639 / \n",
      "========== fold: 0 stage: 1 result ==========\n",
      "Score with best loss weights stage1: 0.4384\n",
      "========== stage: 2 fold: 0 training 79 / 10 ==========\n",
      "Epoch 1 Avg Train Loss: 0.4115 Avg Valid Loss: 0.4625 / \n",
      "Epoch 1 Save Best Valid Loss: 0.4625\n",
      "Epoch 2 Avg Train Loss: 0.3960 Avg Valid Loss: 0.4759 / \n",
      "Epoch 3 Avg Train Loss: 0.3869 Avg Valid Loss: 0.5015 / \n",
      "Epoch 4 Avg Train Loss: 0.3763 Avg Valid Loss: 0.4865 / \n",
      "Epoch 5 Avg Train Loss: 0.3759 Avg Valid Loss: 0.4908 / \n",
      "Epoch 6 Avg Train Loss: 0.3716 Avg Valid Loss: 0.4569 / \n",
      "Epoch 6 Save Best Valid Loss: 0.4569\n",
      "Epoch 7 Avg Train Loss: 0.3702 Avg Valid Loss: 0.4721 / \n",
      "Epoch 8 Avg Train Loss: 0.3664 Avg Valid Loss: 0.4929 / \n",
      "Epoch 9 Avg Train Loss: 0.3705 Avg Valid Loss: 0.4657 / \n",
      "Epoch 10 Avg Train Loss: 0.3622 Avg Valid Loss: 0.4815 / \n",
      "Epoch 11 Avg Train Loss: 0.3595 Avg Valid Loss: 0.4881 / \n",
      "Epoch 12 Avg Train Loss: 0.3588 Avg Valid Loss: 0.5403 / \n",
      "Epoch 13 Avg Train Loss: 0.3621 Avg Valid Loss: 0.4974 / \n",
      "Epoch 14 Avg Train Loss: 0.3567 Avg Valid Loss: 0.4700 / \n",
      "Epoch 15 Avg Train Loss: 0.3551 Avg Valid Loss: 0.4581 / \n",
      "Epoch 16 Avg Train Loss: 0.3578 Avg Valid Loss: 0.4639 / \n",
      "Epoch 17 Avg Train Loss: 0.3538 Avg Valid Loss: 0.4926 / \n",
      "Epoch 18 Avg Train Loss: 0.3577 Avg Valid Loss: 0.4723 / \n",
      "Epoch 19 Avg Train Loss: 0.3599 Avg Valid Loss: 0.4913 / \n",
      "Epoch 20 Avg Train Loss: 0.3564 Avg Valid Loss: 0.5109 / \n",
      "Epoch 21 Avg Train Loss: 0.3475 Avg Valid Loss: 0.5126 / \n",
      "Epoch 22 Avg Train Loss: 0.3581 Avg Valid Loss: 0.4969 / \n",
      "Epoch 23 Avg Train Loss: 0.3516 Avg Valid Loss: 0.4915 / \n",
      "Epoch 24 Avg Train Loss: 0.3462 Avg Valid Loss: 0.5047 / \n",
      "Epoch 25 Avg Train Loss: 0.3527 Avg Valid Loss: 0.4671 / \n",
      "Epoch 26 Avg Train Loss: 0.3466 Avg Valid Loss: 0.4929 / \n",
      "Epoch 27 Avg Train Loss: 0.3453 Avg Valid Loss: 0.4682 / \n",
      "Epoch 28 Avg Train Loss: 0.3447 Avg Valid Loss: 0.4467 / \n",
      "Epoch 28 Save Best Valid Loss: 0.4467\n",
      "Epoch 29 Avg Train Loss: 0.3385 Avg Valid Loss: 0.4656 / \n",
      "Epoch 30 Avg Train Loss: 0.3467 Avg Valid Loss: 0.4390 / \n",
      "Epoch 30 Save Best Valid Loss: 0.4390\n",
      "Epoch 31 Avg Train Loss: 0.3415 Avg Valid Loss: 0.5161 / \n",
      "Epoch 32 Avg Train Loss: 0.3394 Avg Valid Loss: 0.4722 / \n",
      "Epoch 33 Avg Train Loss: 0.3432 Avg Valid Loss: 0.5361 / \n",
      "Epoch 34 Avg Train Loss: 0.3417 Avg Valid Loss: 0.4889 / \n",
      "Epoch 35 Avg Train Loss: 0.3416 Avg Valid Loss: 0.4893 / \n",
      "Epoch 36 Avg Train Loss: 0.3403 Avg Valid Loss: 0.4648 / \n",
      "Epoch 37 Avg Train Loss: 0.3339 Avg Valid Loss: 0.4734 / \n",
      "Epoch 38 Avg Train Loss: 0.3376 Avg Valid Loss: 0.4587 / \n",
      "Epoch 39 Avg Train Loss: 0.3359 Avg Valid Loss: 0.4530 / \n",
      "Epoch 40 Avg Train Loss: 0.3413 Avg Valid Loss: 0.5095 / \n",
      "Epoch 41 Avg Train Loss: 0.3346 Avg Valid Loss: 0.4679 / \n",
      "Epoch 42 Avg Train Loss: 0.3382 Avg Valid Loss: 0.4951 / \n",
      "Epoch 43 Avg Train Loss: 0.3357 Avg Valid Loss: 0.4859 / \n",
      "Epoch 44 Avg Train Loss: 0.3361 Avg Valid Loss: 0.4958 / \n",
      "Epoch 45 Avg Train Loss: 0.3330 Avg Valid Loss: 0.4602 / \n",
      "Epoch 46 Avg Train Loss: 0.3343 Avg Valid Loss: 0.4814 / \n",
      "Epoch 47 Avg Train Loss: 0.3290 Avg Valid Loss: 0.4522 / \n",
      "Epoch 48 Avg Train Loss: 0.3232 Avg Valid Loss: 0.4808 / \n",
      "Epoch 49 Avg Train Loss: 0.3241 Avg Valid Loss: 0.4757 / \n",
      "Epoch 50 Avg Train Loss: 0.3249 Avg Valid Loss: 0.4896 / \n",
      "========== fold: 0 stage: 2 result ==========\n",
      "Score with best loss weights stage2: 0.4390\n",
      "========== stage: 1 fold: 1 training 252 / 32 ==========\n",
      "Epoch 1 Avg Train Loss: 0.6450 Avg Valid Loss: 0.5311 / \n",
      "Epoch 1 Save Best Valid Loss: 0.5311\n",
      "Epoch 2 Avg Train Loss: 0.5116 Avg Valid Loss: 0.5044 / \n",
      "Epoch 2 Save Best Valid Loss: 0.5044\n",
      "Epoch 3 Avg Train Loss: 0.4844 Avg Valid Loss: 0.4929 / \n",
      "Epoch 3 Save Best Valid Loss: 0.4929\n",
      "Epoch 4 Avg Train Loss: 0.4691 Avg Valid Loss: 0.4899 / \n",
      "Epoch 4 Save Best Valid Loss: 0.4899\n",
      "Epoch 5 Avg Train Loss: 0.4582 Avg Valid Loss: 0.4687 / \n",
      "Epoch 5 Save Best Valid Loss: 0.4687\n",
      "Epoch 6 Avg Train Loss: 0.4530 Avg Valid Loss: 0.4702 / \n",
      "Epoch 7 Avg Train Loss: 0.4408 Avg Valid Loss: 0.4691 / \n",
      "Epoch 8 Avg Train Loss: 0.4408 Avg Valid Loss: 0.4396 / \n",
      "Epoch 8 Save Best Valid Loss: 0.4396\n",
      "Epoch 9 Avg Train Loss: 0.4328 Avg Valid Loss: 0.4799 / \n",
      "Epoch 10 Avg Train Loss: 0.4297 Avg Valid Loss: 0.4484 / \n",
      "Epoch 11 Avg Train Loss: 0.4294 Avg Valid Loss: 0.4387 / \n",
      "Epoch 11 Save Best Valid Loss: 0.4387\n",
      "Epoch 12 Avg Train Loss: 0.4248 Avg Valid Loss: 0.4354 / \n",
      "Epoch 12 Save Best Valid Loss: 0.4354\n",
      "Epoch 13 Avg Train Loss: 0.4164 Avg Valid Loss: 0.4569 / \n",
      "Epoch 14 Avg Train Loss: 0.4171 Avg Valid Loss: 0.4730 / \n",
      "Epoch 15 Avg Train Loss: 0.4142 Avg Valid Loss: 0.4616 / \n",
      "Epoch 16 Avg Train Loss: 0.4093 Avg Valid Loss: 0.5031 / \n",
      "Epoch 17 Avg Train Loss: 0.4102 Avg Valid Loss: 0.4229 / \n",
      "Epoch 17 Save Best Valid Loss: 0.4229\n",
      "Epoch 18 Avg Train Loss: 0.4061 Avg Valid Loss: 0.4576 / \n",
      "Epoch 19 Avg Train Loss: 0.4045 Avg Valid Loss: 0.4426 / \n",
      "Epoch 20 Avg Train Loss: 0.4107 Avg Valid Loss: 0.4512 / \n",
      "Epoch 21 Avg Train Loss: 0.4031 Avg Valid Loss: 0.4493 / \n",
      "Epoch 22 Avg Train Loss: 0.4035 Avg Valid Loss: 0.4602 / \n",
      "Epoch 23 Avg Train Loss: 0.4028 Avg Valid Loss: 0.4305 / \n",
      "Epoch 24 Avg Train Loss: 0.4020 Avg Valid Loss: 0.4311 / \n",
      "Epoch 25 Avg Train Loss: 0.3972 Avg Valid Loss: 0.4254 / \n",
      "Epoch 26 Avg Train Loss: 0.3970 Avg Valid Loss: 0.4634 / \n",
      "Epoch 27 Avg Train Loss: 0.3964 Avg Valid Loss: 0.4579 / \n",
      "Epoch 28 Avg Train Loss: 0.3998 Avg Valid Loss: 0.4619 / \n",
      "Epoch 29 Avg Train Loss: 0.3943 Avg Valid Loss: 0.4474 / \n",
      "Epoch 30 Avg Train Loss: 0.3916 Avg Valid Loss: 0.4325 / \n",
      "Epoch 31 Avg Train Loss: 0.3942 Avg Valid Loss: 0.4337 / \n",
      "Epoch 32 Avg Train Loss: 0.3928 Avg Valid Loss: 0.4412 / \n",
      "Epoch 33 Avg Train Loss: 0.3888 Avg Valid Loss: 0.4730 / \n",
      "Epoch 34 Avg Train Loss: 0.3917 Avg Valid Loss: 0.4443 / \n",
      "Epoch 35 Avg Train Loss: 0.3908 Avg Valid Loss: 0.4205 / \n",
      "Epoch 35 Save Best Valid Loss: 0.4205\n",
      "Epoch 36 Avg Train Loss: 0.3880 Avg Valid Loss: 0.4471 / \n",
      "Epoch 37 Avg Train Loss: 0.3890 Avg Valid Loss: 0.4353 / \n",
      "Epoch 38 Avg Train Loss: 0.3881 Avg Valid Loss: 0.4588 / \n",
      "Epoch 39 Avg Train Loss: 0.3905 Avg Valid Loss: 0.4581 / \n",
      "Epoch 40 Avg Train Loss: 0.3873 Avg Valid Loss: 0.4740 / \n",
      "Epoch 41 Avg Train Loss: 0.3867 Avg Valid Loss: 0.4458 / \n",
      "Epoch 42 Avg Train Loss: 0.3812 Avg Valid Loss: 0.4422 / \n",
      "Epoch 43 Avg Train Loss: 0.3790 Avg Valid Loss: 0.4389 / \n",
      "Epoch 44 Avg Train Loss: 0.3815 Avg Valid Loss: 0.4395 / \n",
      "Epoch 45 Avg Train Loss: 0.3793 Avg Valid Loss: 0.4408 / \n",
      "Epoch 46 Avg Train Loss: 0.3804 Avg Valid Loss: 0.4438 / \n",
      "Epoch 47 Avg Train Loss: 0.3796 Avg Valid Loss: 0.4412 / \n",
      "Epoch 48 Avg Train Loss: 0.3815 Avg Valid Loss: 0.4449 / \n",
      "Epoch 49 Avg Train Loss: 0.3787 Avg Valid Loss: 0.4440 / \n",
      "Epoch 50 Avg Train Loss: 0.3797 Avg Valid Loss: 0.4430 / \n",
      "========== fold: 1 stage: 1 result ==========\n",
      "Score with best loss weights stage1: 0.4205\n",
      "========== stage: 2 fold: 1 training 79 / 10 ==========\n",
      "Epoch 1 Avg Train Loss: 0.4102 Avg Valid Loss: 0.3611 / \n",
      "Epoch 1 Save Best Valid Loss: 0.3611\n",
      "Epoch 2 Avg Train Loss: 0.3976 Avg Valid Loss: 0.3706 / \n",
      "Epoch 3 Avg Train Loss: 0.3797 Avg Valid Loss: 0.3782 / \n",
      "Epoch 4 Avg Train Loss: 0.3813 Avg Valid Loss: 0.3806 / \n",
      "Epoch 5 Avg Train Loss: 0.3758 Avg Valid Loss: 0.3667 / \n",
      "Epoch 6 Avg Train Loss: 0.3635 Avg Valid Loss: 0.3616 / \n",
      "Epoch 7 Avg Train Loss: 0.3687 Avg Valid Loss: 0.3833 / \n",
      "Epoch 8 Avg Train Loss: 0.3724 Avg Valid Loss: 0.3740 / \n",
      "Epoch 9 Avg Train Loss: 0.3713 Avg Valid Loss: 0.3737 / \n",
      "Epoch 10 Avg Train Loss: 0.3616 Avg Valid Loss: 0.3680 / \n",
      "Epoch 11 Avg Train Loss: 0.3581 Avg Valid Loss: 0.3859 / \n",
      "Epoch 12 Avg Train Loss: 0.3611 Avg Valid Loss: 0.4065 / \n",
      "Epoch 13 Avg Train Loss: 0.3562 Avg Valid Loss: 0.3909 / \n",
      "Epoch 14 Avg Train Loss: 0.3637 Avg Valid Loss: 0.4230 / \n",
      "Epoch 15 Avg Train Loss: 0.3624 Avg Valid Loss: 0.3909 / \n",
      "Epoch 16 Avg Train Loss: 0.3576 Avg Valid Loss: 0.3844 / \n",
      "Epoch 17 Avg Train Loss: 0.3513 Avg Valid Loss: 0.3912 / \n",
      "Epoch 18 Avg Train Loss: 0.3574 Avg Valid Loss: 0.3921 / \n",
      "Epoch 19 Avg Train Loss: 0.3526 Avg Valid Loss: 0.3834 / \n",
      "Epoch 20 Avg Train Loss: 0.3551 Avg Valid Loss: 0.3771 / \n",
      "Epoch 21 Avg Train Loss: 0.3530 Avg Valid Loss: 0.3943 / \n",
      "Epoch 22 Avg Train Loss: 0.3450 Avg Valid Loss: 0.3897 / \n",
      "Epoch 23 Avg Train Loss: 0.3444 Avg Valid Loss: 0.3861 / \n",
      "Epoch 24 Avg Train Loss: 0.3451 Avg Valid Loss: 0.3857 / \n",
      "Epoch 25 Avg Train Loss: 0.3469 Avg Valid Loss: 0.3803 / \n",
      "Epoch 26 Avg Train Loss: 0.3435 Avg Valid Loss: 0.3797 / \n",
      "Epoch 27 Avg Train Loss: 0.3392 Avg Valid Loss: 0.3824 / \n",
      "Epoch 28 Avg Train Loss: 0.3483 Avg Valid Loss: 0.3792 / \n",
      "Epoch 29 Avg Train Loss: 0.3513 Avg Valid Loss: 0.3838 / \n",
      "Epoch 30 Avg Train Loss: 0.3480 Avg Valid Loss: 0.3818 / \n",
      "Epoch 31 Avg Train Loss: 0.3459 Avg Valid Loss: 0.3805 / \n",
      "Epoch 32 Avg Train Loss: 0.3493 Avg Valid Loss: 0.3825 / \n",
      "Epoch 33 Avg Train Loss: 0.3489 Avg Valid Loss: 0.3811 / \n",
      "Epoch 34 Avg Train Loss: 0.3444 Avg Valid Loss: 0.3810 / \n",
      "Epoch 35 Avg Train Loss: 0.3429 Avg Valid Loss: 0.3860 / \n",
      "Epoch 36 Avg Train Loss: 0.3470 Avg Valid Loss: 0.3806 / \n",
      "Epoch 37 Avg Train Loss: 0.3486 Avg Valid Loss: 0.3816 / \n",
      "Epoch 38 Avg Train Loss: 0.3454 Avg Valid Loss: 0.3819 / \n",
      "Epoch 39 Avg Train Loss: 0.3389 Avg Valid Loss: 0.3804 / \n",
      "Epoch 40 Avg Train Loss: 0.3463 Avg Valid Loss: 0.3816 / \n",
      "Epoch 41 Avg Train Loss: 0.3519 Avg Valid Loss: 0.3813 / \n",
      "Epoch 42 Avg Train Loss: 0.3458 Avg Valid Loss: 0.3794 / \n",
      "Epoch 43 Avg Train Loss: 0.3408 Avg Valid Loss: 0.3803 / \n",
      "Epoch 44 Avg Train Loss: 0.3458 Avg Valid Loss: 0.3826 / \n",
      "Epoch 45 Avg Train Loss: 0.3489 Avg Valid Loss: 0.3821 / \n",
      "Epoch 46 Avg Train Loss: 0.3465 Avg Valid Loss: 0.3850 / \n",
      "Epoch 47 Avg Train Loss: 0.3494 Avg Valid Loss: 0.3818 / \n",
      "Epoch 48 Avg Train Loss: 0.3439 Avg Valid Loss: 0.3808 / \n",
      "Epoch 49 Avg Train Loss: 0.3411 Avg Valid Loss: 0.3794 / \n",
      "Epoch 50 Avg Train Loss: 0.3419 Avg Valid Loss: 0.3799 / \n",
      "========== fold: 1 stage: 2 result ==========\n",
      "Score with best loss weights stage2: 0.3611\n",
      "========== stage: 1 fold: 2 training 252 / 32 ==========\n",
      "Epoch 1 Avg Train Loss: 0.6113 Avg Valid Loss: 0.5982 / \n",
      "Epoch 1 Save Best Valid Loss: 0.5982\n",
      "Epoch 2 Avg Train Loss: 0.5059 Avg Valid Loss: 0.5501 / \n",
      "Epoch 2 Save Best Valid Loss: 0.5501\n",
      "Epoch 3 Avg Train Loss: 0.4809 Avg Valid Loss: 0.4946 / \n",
      "Epoch 3 Save Best Valid Loss: 0.4946\n",
      "Epoch 4 Avg Train Loss: 0.4616 Avg Valid Loss: 0.4705 / \n",
      "Epoch 4 Save Best Valid Loss: 0.4705\n",
      "Epoch 5 Avg Train Loss: 0.4502 Avg Valid Loss: 0.5004 / \n",
      "Epoch 6 Avg Train Loss: 0.4448 Avg Valid Loss: 0.4772 / \n",
      "Epoch 7 Avg Train Loss: 0.4367 Avg Valid Loss: 0.4806 / \n",
      "Epoch 8 Avg Train Loss: 0.4326 Avg Valid Loss: 0.4813 / \n",
      "Epoch 9 Avg Train Loss: 0.4277 Avg Valid Loss: 0.4817 / \n",
      "Epoch 10 Avg Train Loss: 0.4218 Avg Valid Loss: 0.4498 / \n",
      "Epoch 10 Save Best Valid Loss: 0.4498\n",
      "Epoch 11 Avg Train Loss: 0.4214 Avg Valid Loss: 0.4572 / \n",
      "Epoch 12 Avg Train Loss: 0.4161 Avg Valid Loss: 0.4767 / \n",
      "Epoch 13 Avg Train Loss: 0.4166 Avg Valid Loss: 0.4820 / \n",
      "Epoch 14 Avg Train Loss: 0.4100 Avg Valid Loss: 0.4589 / \n",
      "Epoch 15 Avg Train Loss: 0.4111 Avg Valid Loss: 0.4514 / \n",
      "Epoch 16 Avg Train Loss: 0.4082 Avg Valid Loss: 0.4494 / \n",
      "Epoch 16 Save Best Valid Loss: 0.4494\n",
      "Epoch 17 Avg Train Loss: 0.4084 Avg Valid Loss: 0.4796 / \n",
      "Epoch 18 Avg Train Loss: 0.4032 Avg Valid Loss: 0.4684 / \n",
      "Epoch 19 Avg Train Loss: 0.4011 Avg Valid Loss: 0.5188 / \n",
      "Epoch 20 Avg Train Loss: 0.4013 Avg Valid Loss: 0.4436 / \n",
      "Epoch 20 Save Best Valid Loss: 0.4436\n",
      "Epoch 21 Avg Train Loss: 0.3994 Avg Valid Loss: 0.4645 / \n",
      "Epoch 22 Avg Train Loss: 0.3978 Avg Valid Loss: 0.4422 / \n",
      "Epoch 22 Save Best Valid Loss: 0.4422\n",
      "Epoch 23 Avg Train Loss: 0.3954 Avg Valid Loss: 0.4408 / \n",
      "Epoch 23 Save Best Valid Loss: 0.4408\n",
      "Epoch 24 Avg Train Loss: 0.3936 Avg Valid Loss: 0.4431 / \n",
      "Epoch 25 Avg Train Loss: 0.3957 Avg Valid Loss: 0.4451 / \n",
      "Epoch 26 Avg Train Loss: 0.3902 Avg Valid Loss: 0.4644 / \n",
      "Epoch 27 Avg Train Loss: 0.3911 Avg Valid Loss: 0.4485 / \n",
      "Epoch 28 Avg Train Loss: 0.3939 Avg Valid Loss: 0.4536 / \n",
      "Epoch 29 Avg Train Loss: 0.3926 Avg Valid Loss: 0.4544 / \n",
      "Epoch 30 Avg Train Loss: 0.3880 Avg Valid Loss: 0.4302 / \n",
      "Epoch 30 Save Best Valid Loss: 0.4302\n",
      "Epoch 31 Avg Train Loss: 0.3830 Avg Valid Loss: 0.4369 / \n",
      "Epoch 32 Avg Train Loss: 0.3875 Avg Valid Loss: 0.4269 / \n",
      "Epoch 32 Save Best Valid Loss: 0.4269\n",
      "Epoch 33 Avg Train Loss: 0.3883 Avg Valid Loss: 0.4446 / \n",
      "Epoch 34 Avg Train Loss: 0.3891 Avg Valid Loss: 0.4466 / \n",
      "Epoch 35 Avg Train Loss: 0.3844 Avg Valid Loss: 0.4411 / \n",
      "Epoch 36 Avg Train Loss: 0.3835 Avg Valid Loss: 0.4433 / \n",
      "Epoch 37 Avg Train Loss: 0.3797 Avg Valid Loss: 0.4340 / \n",
      "Epoch 38 Avg Train Loss: 0.3789 Avg Valid Loss: 0.4386 / \n",
      "Epoch 39 Avg Train Loss: 0.3822 Avg Valid Loss: 0.4399 / \n",
      "Epoch 40 Avg Train Loss: 0.3798 Avg Valid Loss: 0.4348 / \n",
      "Epoch 41 Avg Train Loss: 0.3774 Avg Valid Loss: 0.4537 / \n",
      "Epoch 42 Avg Train Loss: 0.3827 Avg Valid Loss: 0.4515 / \n",
      "Epoch 43 Avg Train Loss: 0.3791 Avg Valid Loss: 0.4542 / \n",
      "Epoch 44 Avg Train Loss: 0.3781 Avg Valid Loss: 0.4590 / \n",
      "Epoch 45 Avg Train Loss: 0.3778 Avg Valid Loss: 0.4257 / \n",
      "Epoch 45 Save Best Valid Loss: 0.4257\n",
      "Epoch 46 Avg Train Loss: 0.3757 Avg Valid Loss: 0.4458 / \n",
      "Epoch 47 Avg Train Loss: 0.3737 Avg Valid Loss: 0.4539 / \n",
      "Epoch 48 Avg Train Loss: 0.3738 Avg Valid Loss: 0.4479 / \n",
      "Epoch 49 Avg Train Loss: 0.3740 Avg Valid Loss: 0.4542 / \n",
      "Epoch 50 Avg Train Loss: 0.3755 Avg Valid Loss: 0.4640 / \n",
      "========== fold: 2 stage: 1 result ==========\n",
      "Score with best loss weights stage1: 0.4257\n",
      "========== stage: 2 fold: 2 training 79 / 10 ==========\n",
      "Epoch 1 Avg Train Loss: 0.4022 Avg Valid Loss: 0.3736 / \n",
      "Epoch 1 Save Best Valid Loss: 0.3736\n",
      "Epoch 2 Avg Train Loss: 0.3804 Avg Valid Loss: 0.3629 / \n",
      "Epoch 2 Save Best Valid Loss: 0.3629\n",
      "Epoch 3 Avg Train Loss: 0.3717 Avg Valid Loss: 0.3797 / \n",
      "Epoch 4 Avg Train Loss: 0.3732 Avg Valid Loss: 0.3843 / \n",
      "Epoch 5 Avg Train Loss: 0.3668 Avg Valid Loss: 0.3891 / \n",
      "Epoch 6 Avg Train Loss: 0.3638 Avg Valid Loss: 0.3854 / \n",
      "Epoch 7 Avg Train Loss: 0.3631 Avg Valid Loss: 0.3858 / \n",
      "Epoch 8 Avg Train Loss: 0.3652 Avg Valid Loss: 0.3883 / \n",
      "Epoch 9 Avg Train Loss: 0.3641 Avg Valid Loss: 0.3936 / \n",
      "Epoch 10 Avg Train Loss: 0.3732 Avg Valid Loss: 0.3827 / \n",
      "Epoch 11 Avg Train Loss: 0.3698 Avg Valid Loss: 0.3867 / \n",
      "Epoch 12 Avg Train Loss: 0.3648 Avg Valid Loss: 0.3901 / \n",
      "Epoch 13 Avg Train Loss: 0.3666 Avg Valid Loss: 0.3924 / \n",
      "Epoch 14 Avg Train Loss: 0.3626 Avg Valid Loss: 0.3836 / \n",
      "Epoch 15 Avg Train Loss: 0.3724 Avg Valid Loss: 0.3792 / \n",
      "Epoch 16 Avg Train Loss: 0.3645 Avg Valid Loss: 0.3897 / \n",
      "Epoch 17 Avg Train Loss: 0.3702 Avg Valid Loss: 0.3898 / \n",
      "Epoch 18 Avg Train Loss: 0.3638 Avg Valid Loss: 0.3905 / \n",
      "Epoch 19 Avg Train Loss: 0.3622 Avg Valid Loss: 0.3868 / \n",
      "Epoch 20 Avg Train Loss: 0.3682 Avg Valid Loss: 0.3837 / \n",
      "Epoch 21 Avg Train Loss: 0.3706 Avg Valid Loss: 0.3822 / \n",
      "Epoch 22 Avg Train Loss: 0.3669 Avg Valid Loss: 0.3863 / \n",
      "Epoch 23 Avg Train Loss: 0.3620 Avg Valid Loss: 0.3860 / \n",
      "Epoch 24 Avg Train Loss: 0.3709 Avg Valid Loss: 0.3896 / \n",
      "Epoch 25 Avg Train Loss: 0.3696 Avg Valid Loss: 0.3831 / \n",
      "Epoch 26 Avg Train Loss: 0.3728 Avg Valid Loss: 0.3823 / \n",
      "Epoch 27 Avg Train Loss: 0.3673 Avg Valid Loss: 0.3802 / \n",
      "Epoch 28 Avg Train Loss: 0.3669 Avg Valid Loss: 0.3804 / \n",
      "Epoch 29 Avg Train Loss: 0.3648 Avg Valid Loss: 0.3802 / \n",
      "Epoch 30 Avg Train Loss: 0.3717 Avg Valid Loss: 0.3791 / \n",
      "Epoch 31 Avg Train Loss: 0.3710 Avg Valid Loss: 0.3845 / \n",
      "Epoch 32 Avg Train Loss: 0.3643 Avg Valid Loss: 0.3835 / \n",
      "Epoch 33 Avg Train Loss: 0.3691 Avg Valid Loss: 0.4008 / \n",
      "Epoch 34 Avg Train Loss: 0.3646 Avg Valid Loss: 0.3814 / \n",
      "Epoch 35 Avg Train Loss: 0.3659 Avg Valid Loss: 0.3873 / \n",
      "Epoch 36 Avg Train Loss: 0.3690 Avg Valid Loss: 0.3850 / \n",
      "Epoch 37 Avg Train Loss: 0.3691 Avg Valid Loss: 0.3837 / \n",
      "Epoch 38 Avg Train Loss: 0.3680 Avg Valid Loss: 0.3838 / \n",
      "Epoch 39 Avg Train Loss: 0.3694 Avg Valid Loss: 0.3758 / \n",
      "Epoch 40 Avg Train Loss: 0.3693 Avg Valid Loss: 0.3898 / \n",
      "Epoch 41 Avg Train Loss: 0.3698 Avg Valid Loss: 0.3796 / \n",
      "Epoch 42 Avg Train Loss: 0.3700 Avg Valid Loss: 0.3829 / \n",
      "Epoch 43 Avg Train Loss: 0.3623 Avg Valid Loss: 0.3830 / \n",
      "Epoch 44 Avg Train Loss: 0.3701 Avg Valid Loss: 0.3822 / \n",
      "Epoch 45 Avg Train Loss: 0.3696 Avg Valid Loss: 0.3823 / \n",
      "Epoch 46 Avg Train Loss: 0.3683 Avg Valid Loss: 0.3849 / \n",
      "Epoch 47 Avg Train Loss: 0.3717 Avg Valid Loss: 0.3894 / \n",
      "Epoch 48 Avg Train Loss: 0.3633 Avg Valid Loss: 0.3825 / \n",
      "Epoch 49 Avg Train Loss: 0.3657 Avg Valid Loss: 0.3871 / \n",
      "Epoch 50 Avg Train Loss: 0.3646 Avg Valid Loss: 0.3938 / \n",
      "========== fold: 2 stage: 2 result ==========\n",
      "Score with best loss weights stage2: 0.3629\n",
      "========== stage: 1 fold: 3 training 252 / 32 ==========\n",
      "Epoch 1 Avg Train Loss: 0.6240 Avg Valid Loss: 0.5147 / \n",
      "Epoch 1 Save Best Valid Loss: 0.5147\n",
      "Epoch 2 Avg Train Loss: 0.5078 Avg Valid Loss: 0.5275 / \n",
      "Epoch 3 Avg Train Loss: 0.4801 Avg Valid Loss: 0.4576 / \n",
      "Epoch 3 Save Best Valid Loss: 0.4576\n",
      "Epoch 4 Avg Train Loss: 0.4642 Avg Valid Loss: 0.4655 / \n",
      "Epoch 5 Avg Train Loss: 0.4564 Avg Valid Loss: 0.4385 / \n",
      "Epoch 5 Save Best Valid Loss: 0.4385\n",
      "Epoch 6 Avg Train Loss: 0.4478 Avg Valid Loss: 0.4537 / \n",
      "Epoch 7 Avg Train Loss: 0.4434 Avg Valid Loss: 0.4834 / \n",
      "Epoch 8 Avg Train Loss: 0.4397 Avg Valid Loss: 0.4264 / \n",
      "Epoch 8 Save Best Valid Loss: 0.4264\n",
      "Epoch 9 Avg Train Loss: 0.4336 Avg Valid Loss: 0.4401 / \n",
      "Epoch 10 Avg Train Loss: 0.4323 Avg Valid Loss: 0.4417 / \n",
      "Epoch 11 Avg Train Loss: 0.4250 Avg Valid Loss: 0.4223 / \n",
      "Epoch 11 Save Best Valid Loss: 0.4223\n",
      "Epoch 12 Avg Train Loss: 0.4219 Avg Valid Loss: 0.4305 / \n",
      "Epoch 13 Avg Train Loss: 0.4148 Avg Valid Loss: 0.4296 / \n",
      "Epoch 14 Avg Train Loss: 0.4187 Avg Valid Loss: 0.4530 / \n",
      "Epoch 15 Avg Train Loss: 0.4192 Avg Valid Loss: 0.4251 / \n",
      "Epoch 16 Avg Train Loss: 0.4141 Avg Valid Loss: 0.4292 / \n",
      "Epoch 17 Avg Train Loss: 0.4102 Avg Valid Loss: 0.4300 / \n",
      "Epoch 18 Avg Train Loss: 0.4068 Avg Valid Loss: 0.4412 / \n",
      "Epoch 19 Avg Train Loss: 0.4081 Avg Valid Loss: 0.4451 / \n",
      "Epoch 20 Avg Train Loss: 0.4064 Avg Valid Loss: 0.4279 / \n",
      "Epoch 21 Avg Train Loss: 0.4048 Avg Valid Loss: 0.4317 / \n",
      "Epoch 22 Avg Train Loss: 0.4012 Avg Valid Loss: 0.4232 / \n",
      "Epoch 23 Avg Train Loss: 0.4036 Avg Valid Loss: 0.4219 / \n",
      "Epoch 23 Save Best Valid Loss: 0.4219\n",
      "Epoch 24 Avg Train Loss: 0.4035 Avg Valid Loss: 0.4426 / \n",
      "Epoch 25 Avg Train Loss: 0.3963 Avg Valid Loss: 0.4248 / \n",
      "Epoch 26 Avg Train Loss: 0.4000 Avg Valid Loss: 0.4363 / \n",
      "Epoch 27 Avg Train Loss: 0.3931 Avg Valid Loss: 0.4275 / \n",
      "Epoch 28 Avg Train Loss: 0.3999 Avg Valid Loss: 0.4189 / \n",
      "Epoch 28 Save Best Valid Loss: 0.4189\n",
      "Epoch 29 Avg Train Loss: 0.3965 Avg Valid Loss: 0.4366 / \n",
      "Epoch 30 Avg Train Loss: 0.3949 Avg Valid Loss: 0.4047 / \n",
      "Epoch 30 Save Best Valid Loss: 0.4047\n",
      "Epoch 31 Avg Train Loss: 0.3947 Avg Valid Loss: 0.4169 / \n",
      "Epoch 32 Avg Train Loss: 0.3893 Avg Valid Loss: 0.3995 / \n",
      "Epoch 32 Save Best Valid Loss: 0.3995\n",
      "Epoch 33 Avg Train Loss: 0.3935 Avg Valid Loss: 0.4107 / \n",
      "Epoch 34 Avg Train Loss: 0.3900 Avg Valid Loss: 0.4274 / \n",
      "Epoch 35 Avg Train Loss: 0.3878 Avg Valid Loss: 0.4154 / \n",
      "Epoch 36 Avg Train Loss: 0.3894 Avg Valid Loss: 0.4084 / \n",
      "Epoch 37 Avg Train Loss: 0.3877 Avg Valid Loss: 0.4045 / \n",
      "Epoch 38 Avg Train Loss: 0.3880 Avg Valid Loss: 0.4094 / \n",
      "Epoch 39 Avg Train Loss: 0.3879 Avg Valid Loss: 0.4353 / \n",
      "Epoch 40 Avg Train Loss: 0.3832 Avg Valid Loss: 0.4040 / \n",
      "Epoch 41 Avg Train Loss: 0.3861 Avg Valid Loss: 0.4261 / \n",
      "Epoch 42 Avg Train Loss: 0.3837 Avg Valid Loss: 0.3976 / \n",
      "Epoch 42 Save Best Valid Loss: 0.3976\n",
      "Epoch 43 Avg Train Loss: 0.3828 Avg Valid Loss: 0.4253 / \n",
      "Epoch 44 Avg Train Loss: 0.3831 Avg Valid Loss: 0.4220 / \n",
      "Epoch 45 Avg Train Loss: 0.3862 Avg Valid Loss: 0.4047 / \n",
      "Epoch 46 Avg Train Loss: 0.3811 Avg Valid Loss: 0.4156 / \n",
      "Epoch 47 Avg Train Loss: 0.3773 Avg Valid Loss: 0.4060 / \n",
      "Epoch 48 Avg Train Loss: 0.3739 Avg Valid Loss: 0.4054 / \n",
      "Epoch 49 Avg Train Loss: 0.3753 Avg Valid Loss: 0.3993 / \n",
      "Epoch 50 Avg Train Loss: 0.3778 Avg Valid Loss: 0.4075 / \n",
      "========== fold: 3 stage: 1 result ==========\n",
      "Score with best loss weights stage1: 0.3976\n",
      "========== stage: 2 fold: 3 training 79 / 10 ==========\n",
      "Epoch 1 Avg Train Loss: 0.4066 Avg Valid Loss: 0.3579 / \n",
      "Epoch 1 Save Best Valid Loss: 0.3579\n",
      "Epoch 2 Avg Train Loss: 0.3789 Avg Valid Loss: 0.3518 / \n",
      "Epoch 2 Save Best Valid Loss: 0.3518\n",
      "Epoch 3 Avg Train Loss: 0.3691 Avg Valid Loss: 0.3433 / \n",
      "Epoch 3 Save Best Valid Loss: 0.3433\n",
      "Epoch 4 Avg Train Loss: 0.3711 Avg Valid Loss: 0.3529 / \n",
      "Epoch 5 Avg Train Loss: 0.3689 Avg Valid Loss: 0.3597 / \n",
      "Epoch 6 Avg Train Loss: 0.3612 Avg Valid Loss: 0.3768 / \n",
      "Epoch 7 Avg Train Loss: 0.3609 Avg Valid Loss: 0.3566 / \n",
      "Epoch 8 Avg Train Loss: 0.3597 Avg Valid Loss: 0.3483 / \n",
      "Epoch 9 Avg Train Loss: 0.3577 Avg Valid Loss: 0.3496 / \n",
      "Epoch 10 Avg Train Loss: 0.3574 Avg Valid Loss: 0.3560 / \n",
      "Epoch 11 Avg Train Loss: 0.3520 Avg Valid Loss: 0.3741 / \n",
      "Epoch 12 Avg Train Loss: 0.3459 Avg Valid Loss: 0.3827 / \n",
      "Epoch 13 Avg Train Loss: 0.3491 Avg Valid Loss: 0.3832 / \n",
      "Epoch 14 Avg Train Loss: 0.3515 Avg Valid Loss: 0.3680 / \n",
      "Epoch 15 Avg Train Loss: 0.3513 Avg Valid Loss: 0.3825 / \n",
      "Epoch 16 Avg Train Loss: 0.3507 Avg Valid Loss: 0.3798 / \n",
      "Epoch 17 Avg Train Loss: 0.3446 Avg Valid Loss: 0.3653 / \n",
      "Epoch 18 Avg Train Loss: 0.3426 Avg Valid Loss: 0.3453 / \n",
      "Epoch 19 Avg Train Loss: 0.3446 Avg Valid Loss: 0.3555 / \n",
      "Epoch 20 Avg Train Loss: 0.3430 Avg Valid Loss: 0.3513 / \n",
      "Epoch 21 Avg Train Loss: 0.3447 Avg Valid Loss: 0.3836 / \n",
      "Epoch 22 Avg Train Loss: 0.3396 Avg Valid Loss: 0.3768 / \n",
      "Epoch 23 Avg Train Loss: 0.3369 Avg Valid Loss: 0.3678 / \n",
      "Epoch 24 Avg Train Loss: 0.3328 Avg Valid Loss: 0.3677 / \n",
      "Epoch 25 Avg Train Loss: 0.3325 Avg Valid Loss: 0.3635 / \n",
      "Epoch 26 Avg Train Loss: 0.3429 Avg Valid Loss: 0.3716 / \n",
      "Epoch 27 Avg Train Loss: 0.3349 Avg Valid Loss: 0.3717 / \n",
      "Epoch 28 Avg Train Loss: 0.3393 Avg Valid Loss: 0.3730 / \n",
      "Epoch 29 Avg Train Loss: 0.3378 Avg Valid Loss: 0.3901 / \n",
      "Epoch 30 Avg Train Loss: 0.3367 Avg Valid Loss: 0.3656 / \n",
      "Epoch 31 Avg Train Loss: 0.3340 Avg Valid Loss: 0.3698 / \n",
      "Epoch 32 Avg Train Loss: 0.3395 Avg Valid Loss: 0.3832 / \n",
      "Epoch 33 Avg Train Loss: 0.3406 Avg Valid Loss: 0.3744 / \n",
      "Epoch 34 Avg Train Loss: 0.3340 Avg Valid Loss: 0.3814 / \n",
      "Epoch 35 Avg Train Loss: 0.3425 Avg Valid Loss: 0.3784 / \n",
      "Epoch 36 Avg Train Loss: 0.3349 Avg Valid Loss: 0.3773 / \n",
      "Epoch 37 Avg Train Loss: 0.3370 Avg Valid Loss: 0.3716 / \n",
      "Epoch 38 Avg Train Loss: 0.3318 Avg Valid Loss: 0.3686 / \n",
      "Epoch 39 Avg Train Loss: 0.3281 Avg Valid Loss: 0.3835 / \n",
      "Epoch 40 Avg Train Loss: 0.3413 Avg Valid Loss: 0.3862 / \n",
      "Epoch 41 Avg Train Loss: 0.3282 Avg Valid Loss: 0.3676 / \n",
      "Epoch 42 Avg Train Loss: 0.3397 Avg Valid Loss: 0.3791 / \n",
      "Epoch 43 Avg Train Loss: 0.3401 Avg Valid Loss: 0.3705 / \n",
      "Epoch 44 Avg Train Loss: 0.3351 Avg Valid Loss: 0.3625 / \n",
      "Epoch 45 Avg Train Loss: 0.3305 Avg Valid Loss: 0.3739 / \n",
      "Epoch 46 Avg Train Loss: 0.3338 Avg Valid Loss: 0.3674 / \n",
      "Epoch 47 Avg Train Loss: 0.3358 Avg Valid Loss: 0.3753 / \n",
      "Epoch 48 Avg Train Loss: 0.3346 Avg Valid Loss: 0.3836 / \n",
      "Epoch 49 Avg Train Loss: 0.3357 Avg Valid Loss: 0.3706 / \n",
      "Epoch 50 Avg Train Loss: 0.3433 Avg Valid Loss: 0.3735 / \n",
      "========== fold: 3 stage: 2 result ==========\n",
      "Score with best loss weights stage2: 0.3433\n",
      "========== stage: 1 fold: 4 training 252 / 32 ==========\n",
      "Epoch 1 Avg Train Loss: 0.6445 Avg Valid Loss: 0.5632 / \n",
      "Epoch 1 Save Best Valid Loss: 0.5632\n",
      "Epoch 2 Avg Train Loss: 0.5186 Avg Valid Loss: 0.5067 / \n",
      "Epoch 2 Save Best Valid Loss: 0.5067\n",
      "Epoch 3 Avg Train Loss: 0.4844 Avg Valid Loss: 0.4786 / \n",
      "Epoch 3 Save Best Valid Loss: 0.4786\n",
      "Epoch 4 Avg Train Loss: 0.4704 Avg Valid Loss: 0.5009 / \n",
      "Epoch 5 Avg Train Loss: 0.4629 Avg Valid Loss: 0.4701 / \n",
      "Epoch 5 Save Best Valid Loss: 0.4701\n",
      "Epoch 6 Avg Train Loss: 0.4489 Avg Valid Loss: 0.4696 / \n",
      "Epoch 6 Save Best Valid Loss: 0.4696\n",
      "Epoch 7 Avg Train Loss: 0.4451 Avg Valid Loss: 0.4793 / \n",
      "Epoch 8 Avg Train Loss: 0.4381 Avg Valid Loss: 0.4752 / \n",
      "Epoch 9 Avg Train Loss: 0.4356 Avg Valid Loss: 0.4687 / \n",
      "Epoch 9 Save Best Valid Loss: 0.4687\n",
      "Epoch 10 Avg Train Loss: 0.4326 Avg Valid Loss: 0.4603 / \n",
      "Epoch 10 Save Best Valid Loss: 0.4603\n",
      "Epoch 11 Avg Train Loss: 0.4304 Avg Valid Loss: 0.4619 / \n",
      "Epoch 12 Avg Train Loss: 0.4233 Avg Valid Loss: 0.4439 / \n",
      "Epoch 12 Save Best Valid Loss: 0.4439\n",
      "Epoch 13 Avg Train Loss: 0.4244 Avg Valid Loss: 0.4397 / \n",
      "Epoch 13 Save Best Valid Loss: 0.4397\n",
      "Epoch 14 Avg Train Loss: 0.4172 Avg Valid Loss: 0.4693 / \n",
      "Epoch 15 Avg Train Loss: 0.4202 Avg Valid Loss: 0.4436 / \n",
      "Epoch 16 Avg Train Loss: 0.4169 Avg Valid Loss: 0.4415 / \n",
      "Epoch 17 Avg Train Loss: 0.4159 Avg Valid Loss: 0.4665 / \n",
      "Epoch 18 Avg Train Loss: 0.4143 Avg Valid Loss: 0.4345 / \n",
      "Epoch 18 Save Best Valid Loss: 0.4345\n",
      "Epoch 19 Avg Train Loss: 0.4113 Avg Valid Loss: 0.4646 / \n",
      "Epoch 20 Avg Train Loss: 0.4106 Avg Valid Loss: 0.4489 / \n",
      "Epoch 21 Avg Train Loss: 0.4027 Avg Valid Loss: 0.4418 / \n",
      "Epoch 22 Avg Train Loss: 0.4045 Avg Valid Loss: 0.4646 / \n",
      "Epoch 23 Avg Train Loss: 0.4058 Avg Valid Loss: 0.4356 / \n",
      "Epoch 24 Avg Train Loss: 0.4048 Avg Valid Loss: 0.4667 / \n",
      "Epoch 25 Avg Train Loss: 0.4036 Avg Valid Loss: 0.4321 / \n",
      "Epoch 25 Save Best Valid Loss: 0.4321\n",
      "Epoch 26 Avg Train Loss: 0.3978 Avg Valid Loss: 0.4711 / \n",
      "Epoch 27 Avg Train Loss: 0.3987 Avg Valid Loss: 0.4540 / \n",
      "Epoch 28 Avg Train Loss: 0.3956 Avg Valid Loss: 0.4369 / \n",
      "Epoch 29 Avg Train Loss: 0.3940 Avg Valid Loss: 0.4447 / \n",
      "Epoch 30 Avg Train Loss: 0.3941 Avg Valid Loss: 0.4270 / \n",
      "Epoch 30 Save Best Valid Loss: 0.4270\n",
      "Epoch 31 Avg Train Loss: 0.3930 Avg Valid Loss: 0.4324 / \n",
      "Epoch 32 Avg Train Loss: 0.3953 Avg Valid Loss: 0.4423 / \n",
      "Epoch 33 Avg Train Loss: 0.3981 Avg Valid Loss: 0.4557 / \n",
      "Epoch 34 Avg Train Loss: 0.3908 Avg Valid Loss: 0.4305 / \n",
      "Epoch 35 Avg Train Loss: 0.3822 Avg Valid Loss: 0.4292 / \n",
      "Epoch 36 Avg Train Loss: 0.3811 Avg Valid Loss: 0.4333 / \n",
      "Epoch 37 Avg Train Loss: 0.3851 Avg Valid Loss: 0.4311 / \n",
      "Epoch 38 Avg Train Loss: 0.3823 Avg Valid Loss: 0.4316 / \n",
      "Epoch 39 Avg Train Loss: 0.3801 Avg Valid Loss: 0.4328 / \n",
      "Epoch 40 Avg Train Loss: 0.3818 Avg Valid Loss: 0.4299 / \n",
      "Epoch 41 Avg Train Loss: 0.3839 Avg Valid Loss: 0.4336 / \n",
      "Epoch 42 Avg Train Loss: 0.3832 Avg Valid Loss: 0.4300 / \n",
      "Epoch 43 Avg Train Loss: 0.3832 Avg Valid Loss: 0.4287 / \n",
      "Epoch 44 Avg Train Loss: 0.3832 Avg Valid Loss: 0.4331 / \n",
      "Epoch 45 Avg Train Loss: 0.3818 Avg Valid Loss: 0.4324 / \n",
      "Epoch 46 Avg Train Loss: 0.3820 Avg Valid Loss: 0.4297 / \n",
      "Epoch 47 Avg Train Loss: 0.3860 Avg Valid Loss: 0.4302 / \n",
      "Epoch 48 Avg Train Loss: 0.3828 Avg Valid Loss: 0.4330 / \n",
      "Epoch 49 Avg Train Loss: 0.3835 Avg Valid Loss: 0.4297 / \n",
      "Epoch 50 Avg Train Loss: 0.3814 Avg Valid Loss: 0.4326 / \n",
      "========== fold: 4 stage: 1 result ==========\n",
      "Score with best loss weights stage1: 0.4270\n",
      "========== stage: 2 fold: 4 training 79 / 10 ==========\n",
      "Epoch 1 Avg Train Loss: 0.4203 Avg Valid Loss: 0.3608 / \n",
      "Epoch 1 Save Best Valid Loss: 0.3608\n",
      "Epoch 2 Avg Train Loss: 0.3883 Avg Valid Loss: 0.3433 / \n",
      "Epoch 2 Save Best Valid Loss: 0.3433\n",
      "Epoch 3 Avg Train Loss: 0.3887 Avg Valid Loss: 0.3537 / \n",
      "Epoch 4 Avg Train Loss: 0.3812 Avg Valid Loss: 0.3491 / \n",
      "Epoch 5 Avg Train Loss: 0.3786 Avg Valid Loss: 0.3531 / \n",
      "Epoch 6 Avg Train Loss: 0.3760 Avg Valid Loss: 0.3818 / \n",
      "Epoch 7 Avg Train Loss: 0.3856 Avg Valid Loss: 0.3603 / \n",
      "Epoch 8 Avg Train Loss: 0.3680 Avg Valid Loss: 0.3953 / \n",
      "Epoch 9 Avg Train Loss: 0.3717 Avg Valid Loss: 0.3474 / \n",
      "Epoch 10 Avg Train Loss: 0.3701 Avg Valid Loss: 0.3548 / \n",
      "Epoch 11 Avg Train Loss: 0.3641 Avg Valid Loss: 0.3465 / \n",
      "Epoch 12 Avg Train Loss: 0.3633 Avg Valid Loss: 0.3552 / \n",
      "Epoch 13 Avg Train Loss: 0.3582 Avg Valid Loss: 0.3448 / \n",
      "Epoch 14 Avg Train Loss: 0.3526 Avg Valid Loss: 0.3560 / \n",
      "Epoch 15 Avg Train Loss: 0.3573 Avg Valid Loss: 0.3755 / \n",
      "Epoch 16 Avg Train Loss: 0.3574 Avg Valid Loss: 0.3554 / \n",
      "Epoch 17 Avg Train Loss: 0.3560 Avg Valid Loss: 0.3456 / \n",
      "Epoch 18 Avg Train Loss: 0.3533 Avg Valid Loss: 0.3546 / \n",
      "Epoch 19 Avg Train Loss: 0.3569 Avg Valid Loss: 0.3529 / \n",
      "Epoch 20 Avg Train Loss: 0.3604 Avg Valid Loss: 0.3566 / \n",
      "Epoch 21 Avg Train Loss: 0.3570 Avg Valid Loss: 0.3516 / \n",
      "Epoch 22 Avg Train Loss: 0.3419 Avg Valid Loss: 0.3482 / \n",
      "Epoch 23 Avg Train Loss: 0.3518 Avg Valid Loss: 0.3496 / \n",
      "Epoch 24 Avg Train Loss: 0.3478 Avg Valid Loss: 0.3710 / \n",
      "Epoch 25 Avg Train Loss: 0.3498 Avg Valid Loss: 0.3508 / \n",
      "Epoch 26 Avg Train Loss: 0.3496 Avg Valid Loss: 0.3572 / \n",
      "Epoch 27 Avg Train Loss: 0.3422 Avg Valid Loss: 0.3563 / \n",
      "Epoch 28 Avg Train Loss: 0.3567 Avg Valid Loss: 0.3791 / \n",
      "Epoch 29 Avg Train Loss: 0.3454 Avg Valid Loss: 0.3785 / \n",
      "Epoch 30 Avg Train Loss: 0.3455 Avg Valid Loss: 0.3529 / \n",
      "Epoch 31 Avg Train Loss: 0.3496 Avg Valid Loss: 0.3728 / \n",
      "Epoch 32 Avg Train Loss: 0.3401 Avg Valid Loss: 0.3569 / \n",
      "Epoch 33 Avg Train Loss: 0.3380 Avg Valid Loss: 0.3937 / \n",
      "Epoch 34 Avg Train Loss: 0.3368 Avg Valid Loss: 0.3718 / \n",
      "Epoch 35 Avg Train Loss: 0.3420 Avg Valid Loss: 0.3864 / \n",
      "Epoch 36 Avg Train Loss: 0.3377 Avg Valid Loss: 0.3708 / \n",
      "Epoch 37 Avg Train Loss: 0.3402 Avg Valid Loss: 0.3792 / \n",
      "Epoch 38 Avg Train Loss: 0.3397 Avg Valid Loss: 0.3637 / \n",
      "Epoch 39 Avg Train Loss: 0.3315 Avg Valid Loss: 0.3632 / \n",
      "Epoch 40 Avg Train Loss: 0.3356 Avg Valid Loss: 0.3639 / \n",
      "Epoch 41 Avg Train Loss: 0.3392 Avg Valid Loss: 0.3620 / \n",
      "Epoch 42 Avg Train Loss: 0.3328 Avg Valid Loss: 0.3644 / \n",
      "Epoch 43 Avg Train Loss: 0.3352 Avg Valid Loss: 0.3945 / \n",
      "Epoch 44 Avg Train Loss: 0.3310 Avg Valid Loss: 0.3720 / \n",
      "Epoch 45 Avg Train Loss: 0.3345 Avg Valid Loss: 0.3651 / \n",
      "Epoch 46 Avg Train Loss: 0.3369 Avg Valid Loss: 0.3577 / \n",
      "Epoch 47 Avg Train Loss: 0.3297 Avg Valid Loss: 0.3556 / \n",
      "Epoch 48 Avg Train Loss: 0.3333 Avg Valid Loss: 0.3674 / \n",
      "Epoch 49 Avg Train Loss: 0.3330 Avg Valid Loss: 0.4003 / \n",
      "Epoch 50 Avg Train Loss: 0.3315 Avg Valid Loss: 0.3971 / \n",
      "========== fold: 4 stage: 2 result ==========\n",
      "Score with best loss weights stage2: 0.3433\n",
      "============ CV score with best loss weights ============\n",
      "Stage 0: 0.4218\n",
      "============ CV score with best loss weights ============\n",
      "Stage 1: 0.3699\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\" and CFG.train_by_folds:\n",
    "    seed_torch(seed=CFG.seed)\n",
    "\n",
    "    stages_scores = {i: [] for i in CFG.train_stages}\n",
    "    stages_oof_df = {i: pd.DataFrame() for i in CFG.train_stages}\n",
    "\n",
    "    for fold in CFG.train_folds:\n",
    "\n",
    "        prev_dir = \"\"\n",
    "        for stage in range(len(CFG.total_evaluators)):\n",
    "\n",
    "            pop_dir = f\"{OUTPUT_DIR}pop_{stage+1}_weight_oof/\"\n",
    "            if not os.path.exists(pop_dir):\n",
    "                os.makedirs(pop_dir)\n",
    "\n",
    "            if stage not in CFG.train_stages:\n",
    "                prev_dir = pop_dir\n",
    "                continue\n",
    "\n",
    "            train_oof_df, score = train_loop(\n",
    "                stage=stage + 1,\n",
    "                epochs=CFG.epochs[stage],\n",
    "                fold=fold,\n",
    "                folds=train_pops[stage],\n",
    "                directory=pop_dir,\n",
    "                prev_dir=prev_dir,\n",
    "                eggs=all_eegs,\n",
    "            )\n",
    "\n",
    "            stages_oof_df[stage] = pd.concat([stages_oof_df[stage], train_oof_df])\n",
    "            stages_scores[stage].append(score)\n",
    "\n",
    "            prev_dir = pop_dir\n",
    "\n",
    "            LOGGER.info(f\"========== fold: {fold} stage: {stage+1} result ==========\")\n",
    "            LOGGER.info(f\"Score with best loss weights stage{stage+1}: {score:.4f}\")\n",
    "\n",
    "    for stage, scores in stages_scores.items():\n",
    "        LOGGER.info(f\"============ CV score with best loss weights ============\")\n",
    "        LOGGER.info(f\"Stage {stage}: {np.mean(scores):.4f}\")\n",
    "\n",
    "    for stage, oof_df in stages_oof_df.items():\n",
    "        pop_dir = f\"{OUTPUT_DIR}pop_{stage+1}_weight_oof/\"\n",
    "        oof_df.reset_index(drop=True, inplace=True)\n",
    "        oof_df.to_csv(\n",
    "            f\"{pop_dir}{CFG.model_name}_oof_df_ver-{CFG.VERSION}_stage-{stage+1}.csv\",\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "    if CFG.wandb:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7548b13",
   "metadata": {
    "papermill": {
     "duration": 0.064334,
     "end_time": "2024-03-12T13:50:40.790602",
     "exception": false,
     "start_time": "2024-03-12T13:50:40.726268",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1dc86eeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T13:50:40.920596Z",
     "iopub.status.busy": "2024-03-12T13:50:40.919677Z",
     "iopub.status.idle": "2024-03-12T13:50:40.975949Z",
     "shell.execute_reply": "2024-03-12T13:50:40.974813Z"
    },
    "papermill": {
     "duration": 0.124044,
     "end_time": "2024-03-12T13:50:40.978359",
     "exception": false,
     "start_time": "2024-03-12T13:50:40.854315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Score with resnet1D_gru Raw EEG = 0.3699\n"
     ]
    }
   ],
   "source": [
    "# === Pre-process OOF ===\n",
    "gt = oof_df[[\"eeg_id\"] + CFG.target_cols]\n",
    "gt.sort_values(by=\"eeg_id\", inplace=True)\n",
    "gt.reset_index(inplace=True, drop=True)\n",
    "\n",
    "preds = oof_df[[\"eeg_id\"] + CFG.pred_cols]\n",
    "preds.columns = [\"eeg_id\"] + CFG.target_cols\n",
    "preds.sort_values(by=\"eeg_id\", inplace=True)\n",
    "preds.reset_index(inplace=True, drop=True)\n",
    "\n",
    "y_trues = gt[CFG.target_cols]\n",
    "y_preds = preds[CFG.target_cols]\n",
    "\n",
    "oof = pd.DataFrame(y_preds.copy())\n",
    "oof[\"id\"] = np.arange(len(oof))\n",
    "\n",
    "true = pd.DataFrame(y_trues.copy())\n",
    "true[\"id\"] = np.arange(len(true))\n",
    "\n",
    "cv = kaggle_kl_div.score(solution=true, submission=oof, row_id_column_name=\"id\")\n",
    "print(f\"CV Score with resnet1D_gru Raw EEG = {cv:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7469972,
     "sourceId": 59093,
     "sourceType": "competition"
    },
    {
     "datasetId": 4297749,
     "sourceId": 7392733,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4317718,
     "sourceId": 7465251,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4378712,
     "sourceId": 7517324,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4527097,
     "sourceId": 7805751,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17343.040241,
   "end_time": "2024-03-12T13:50:44.435596",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-12T09:01:41.395355",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "088fc3f5e4264683bbeaa2d929fab599": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2c3d333449fb412dbc86ca31835ef4b0",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3ba5491929cf4cb1bbb8e28514933aa6",
       "value": 1
      }
     },
     "11bfd3544cf8411e96cd7ede1451518c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5246ec5881c147f09c90846e9911dda6",
       "placeholder": "​",
       "style": "IPY_MODEL_db8fdfb810194e03b66c7c5e5077ccf5",
       "value": " 1/? [00:00&lt;00:00,  1.37it/s]"
      }
     },
     "2c3d333449fb412dbc86ca31835ef4b0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "3ba5491929cf4cb1bbb8e28514933aa6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5246ec5881c147f09c90846e9911dda6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "58eb07f979fd49e8a9155ad60c77923c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "993085d825a542b1b5c44cd42f7f8c89": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c818523154ec45689615162b4c03bf96",
        "IPY_MODEL_088fc3f5e4264683bbeaa2d929fab599",
        "IPY_MODEL_11bfd3544cf8411e96cd7ede1451518c"
       ],
       "layout": "IPY_MODEL_fa70a85db4944741a7d7b13cef19bb70"
      }
     },
     "c818523154ec45689615162b4c03bf96": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_58eb07f979fd49e8a9155ad60c77923c",
       "placeholder": "​",
       "style": "IPY_MODEL_ea4dcc5f4fcf4011a7ba7dc587109a9b",
       "value": ""
      }
     },
     "db8fdfb810194e03b66c7c5e5077ccf5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ea4dcc5f4fcf4011a7ba7dc587109a9b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fa70a85db4944741a7d7b13cef19bb70": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
